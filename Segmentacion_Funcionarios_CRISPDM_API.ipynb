{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Segmentaci√≥n de Funcionarios P√∫blicos a Contrata - Chile 2022\n",
    "\n",
    "## Proyecto de Machine Learning - Clustering\n",
    "\n",
    "**Autor:** Ana Karina Mu√±oz  \n",
    "**Metodolog√≠a:** CRISP-DM  \n",
    "**Fecha:** 2024  \n",
    "\n",
    "---\n",
    "\n",
    "### Descripci√≥n del Proyecto\n",
    "\n",
    "Este proyecto aplica t√©cnicas de **aprendizaje no supervisado (clustering)** para segmentar funcionarios p√∫blicos a contrata de municipalidades chilenas. El objetivo es identificar grupos homog√©neos que permitan a instituciones fiscalizadoras detectar comportamientos an√≥malos.\n",
    "\n",
    "### Metodolog√≠a CRISP-DM\n",
    "\n",
    "Seguiremos las 6 fases de CRISP-DM:\n",
    "\n",
    "1. **Business Understanding** - Entender el problema de negocio\n",
    "2. **Data Understanding** - Explorar y entender los datos\n",
    "3. **Data Preparation** - Limpiar y transformar los datos\n",
    "4. **Modeling** - Entrenar y comparar modelos\n",
    "5. **Evaluation** - Evaluar e interpretar resultados\n",
    "6. **Deployment** - Preparar para producci√≥n\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tabla de Contenidos\n",
    "\n",
    "1. [Business Understanding](#1-business-understanding)\n",
    "2. [Data Understanding](#2-data-understanding)\n",
    "   - 2.1 Conexi√≥n a API datos.gob.cl\n",
    "   - 2.2 Carga de datos\n",
    "   - 2.3 Exploraci√≥n inicial\n",
    "   - 2.4 An√°lisis de distribuciones\n",
    "   - 2.5 Detecci√≥n de outliers\n",
    "3. [Data Preparation](#3-data-preparation)\n",
    "   - 3.1 Limpieza de datos\n",
    "   - 3.2 Feature Engineering\n",
    "   - 3.3 Tratamiento de outliers\n",
    "   - 3.4 Transformaciones\n",
    "   - 3.5 Estandarizaci√≥n\n",
    "4. [Modeling](#4-modeling)\n",
    "   - 4.1 Selecci√≥n de K √≥ptimo\n",
    "   - 4.2 K-Means\n",
    "   - 4.3 DBSCAN\n",
    "   - 4.4 OPTICS\n",
    "   - 4.5 Comparaci√≥n de modelos\n",
    "5. [Evaluation](#5-evaluation)\n",
    "   - 5.1 M√©tricas de clustering\n",
    "   - 5.2 Interpretaci√≥n de clusters\n",
    "   - 5.3 Hallazgos clave\n",
    "6. [Deployment](#6-deployment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 1. Business Understanding\n",
    "\n",
    "## 1.1 Contexto del Problema\n",
    "\n",
    "La **transparencia en el sector p√∫blico** es un desaf√≠o persistente en Chile:\n",
    "\n",
    "- El 40% de las municipalidades enfrenta querellas por falta de transparencia (Ciper, 2023)\n",
    "- El 76% de los chilenos percibe alto nivel de corrupci√≥n (IPSOS, 2023)\n",
    "- La Ley de Transparencia exige publicar datos de remuneraciones de funcionarios p√∫blicos\n",
    "\n",
    "## 1.2 Objetivo del Proyecto\n",
    "\n",
    "Desarrollar un modelo de **segmentaci√≥n de funcionarios p√∫blicos** que permita:\n",
    "\n",
    "1. Identificar grupos homog√©neos seg√∫n remuneraci√≥n, antig√ºedad y cargo\n",
    "2. Detectar patrones an√≥malos (ej: salarios at√≠picos para un cargo)\n",
    "3. Facilitar la fiscalizaci√≥n por parte de instituciones pertinentes\n",
    "\n",
    "## 1.3 Stakeholders\n",
    "\n",
    "| Stakeholder | Inter√©s | Uso del modelo |\n",
    "|-------------|---------|----------------|\n",
    "| Contralor√≠a | Fiscalizaci√≥n | Priorizar auditor√≠as |\n",
    "| Ciudadan√≠a | Transparencia | Consulta p√∫blica |\n",
    "| Bancos | Evaluaci√≥n de riesgo | Perfilamiento de clientes |\n",
    "\n",
    "## 1.4 Criterios de √âxito\n",
    "\n",
    "- Silhouette Score > 0.25 (clusters bien separados)\n",
    "- Clusters interpretables y accionables\n",
    "- Modelo reproducible y documentado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# CONFIGURACI√ìN INICIAL Y CARGA DE LIBRER√çAS\n",
    "# ==============================================================================\n",
    "\n",
    "# Librer√≠as est√°ndar de Python\n",
    "import warnings\n",
    "import json\n",
    "import time\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from io import StringIO\n",
    "\n",
    "# Librer√≠as de an√°lisis de datos\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Librer√≠as de visualizaci√≥n\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Librer√≠as de Machine Learning (sklearn)\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "from sklearn.cluster import KMeans, DBSCAN, OPTICS\n",
    "from sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "# Librer√≠as para estad√≠sticas\n",
    "from scipy import stats\n",
    "from scipy.stats import zscore\n",
    "\n",
    "# Para guardar modelos\n",
    "import joblib\n",
    "\n",
    "# Para conexi√≥n a API\n",
    "import requests\n",
    "\n",
    "# ==============================================================================\n",
    "# CONFIGURACI√ìN GLOBAL\n",
    "# ==============================================================================\n",
    "\n",
    "# Ignorar warnings para mantener el notebook limpio\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Estilo de gr√°ficos\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_palette('husl')\n",
    "\n",
    "# Configuraci√≥n de pandas para mejor visualizaci√≥n\n",
    "pd.set_option('display.max_columns', 50)\n",
    "pd.set_option('display.float_format', '{:,.2f}'.format)\n",
    "\n",
    "# Semilla para reproducibilidad\n",
    "# IMPORTANTE: Siempre usar semilla fija para que los resultados sean replicables\n",
    "RANDOM_STATE = 42\n",
    "np.random.seed(RANDOM_STATE)\n",
    "\n",
    "print(\"‚úì Librer√≠as cargadas correctamente\")\n",
    "print(f\"  - Pandas: {pd.__version__}\")\n",
    "print(f\"  - NumPy: {np.__version__}\")\n",
    "print(f\"  - Semilla aleatoria: {RANDOM_STATE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# CONFIGURACI√ìN DEL PROYECTO\n",
    "# ==============================================================================\n",
    "\n",
    "# Crear estructura de directorios\n",
    "BASE_DIR = Path.cwd()\n",
    "DATA_DIR = BASE_DIR / 'data'\n",
    "RAW_DIR = DATA_DIR / 'raw'\n",
    "PROCESSED_DIR = DATA_DIR / 'processed'\n",
    "MODELS_DIR = BASE_DIR / 'models'\n",
    "REPORTS_DIR = BASE_DIR / 'reports'\n",
    "\n",
    "# Crear directorios si no existen\n",
    "for directory in [RAW_DIR, PROCESSED_DIR, MODELS_DIR, REPORTS_DIR]:\n",
    "    directory.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Par√°metros del proyecto\n",
    "CONFIG = {\n",
    "    'USE_CACHE': True,\n",
    "    'YEAR_FILTER': 2022,\n",
    "    'OUTLIER_METHOD': 'iqr',\n",
    "    'OUTLIER_THRESHOLD': 1.5,\n",
    "    'LOG_TRANSFORM': True,\n",
    "    'N_CLUSTERS_RANGE': (2, 12),\n",
    "}\n",
    "\n",
    "print(\"‚úì Configuraci√≥n del proyecto:\")\n",
    "print(f\"  - Directorio base: {BASE_DIR}\")\n",
    "print(f\"  - A√±o de an√°lisis: {CONFIG['YEAR_FILTER']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 2. Data Understanding\n",
    "\n",
    "## 2.1 Conexi√≥n a API datos.gob.cl\n",
    "\n",
    "Los datos provienen del **Portal de Datos Abiertos de Chile** (datos.gob.cl), que utiliza el est√°ndar **CKAN** (Comprehensive Knowledge Archive Network).\n",
    "\n",
    "### Endpoints de la API CKAN:\n",
    "\n",
    "| Endpoint | Descripci√≥n |\n",
    "|----------|-------------|\n",
    "| `package_search` | Buscar datasets por palabra clave |\n",
    "| `package_show` | Obtener metadata de un dataset |\n",
    "| `datastore_search` | Consultar datos de un recurso |\n",
    "| `resource_show` | Obtener info de un recurso espec√≠fico |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# CLASE PARA CONEXI√ìN A API CKAN (datos.gob.cl)\n",
    "# ==============================================================================\n",
    "\n",
    "class DatosFuncionariosChile:\n",
    "    \"\"\"\n",
    "    Clase para cargar datos de funcionarios p√∫blicos desde datos.gob.cl\n",
    "    \n",
    "    La API de datos.gob.cl usa el est√°ndar CKAN (mismo que data.gov de USA).\n",
    "    \n",
    "    Flujo de carga:\n",
    "    1. Intenta cargar desde cache local (m√°s r√°pido)\n",
    "    2. Si no hay cache, conecta a la API de datos.gob.cl\n",
    "    3. Si la API falla, genera datos sint√©ticos para demostraci√≥n\n",
    "    \n",
    "    Attributes:\n",
    "        base_url: URL base de la API CKAN\n",
    "        cache_dir: Directorio para guardar cache\n",
    "        metadata: Informaci√≥n sobre la fuente de datos\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, cache_dir=None):\n",
    "        \"\"\"Inicializa el cliente de la API.\"\"\"\n",
    "        self.base_url = \"https://datos.gob.cl/api/3/action\"\n",
    "        self.cache_dir = Path(cache_dir) if cache_dir else RAW_DIR\n",
    "        self.metadata = {}\n",
    "        self.session = requests.Session()\n",
    "        # Timeout para evitar esperas largas si la API no responde\n",
    "        self.timeout = 30\n",
    "    \n",
    "    def _api_request(self, endpoint, params=None):\n",
    "        \"\"\"\n",
    "        Realiza una petici√≥n a la API CKAN.\n",
    "        \n",
    "        Args:\n",
    "            endpoint: Nombre del endpoint (ej: 'package_search')\n",
    "            params: Diccionario con par√°metros de la petici√≥n\n",
    "            \n",
    "        Returns:\n",
    "            dict: Respuesta de la API en formato JSON\n",
    "        \"\"\"\n",
    "        url = f\"{self.base_url}/{endpoint}\"\n",
    "        try:\n",
    "            response = self.session.get(url, params=params, timeout=self.timeout)\n",
    "            response.raise_for_status()\n",
    "            data = response.json()\n",
    "            \n",
    "            if data.get('success'):\n",
    "                return data.get('result')\n",
    "            else:\n",
    "                print(f\"‚ö†Ô∏è API retorn√≥ error: {data.get('error')}\")\n",
    "                return None\n",
    "        except requests.exceptions.Timeout:\n",
    "            print(f\"‚ö†Ô∏è Timeout al conectar con {url}\")\n",
    "            return None\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"‚ö†Ô∏è Error de conexi√≥n: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def search_datasets(self, query, rows=10):\n",
    "        \"\"\"\n",
    "        Busca datasets en datos.gob.cl por palabra clave.\n",
    "        \n",
    "        Args:\n",
    "            query: T√©rmino de b√∫squeda\n",
    "            rows: N√∫mero m√°ximo de resultados\n",
    "            \n",
    "        Returns:\n",
    "            list: Lista de datasets encontrados\n",
    "        \"\"\"\n",
    "        print(f\"üîç Buscando datasets con: '{query}'...\")\n",
    "        result = self._api_request('package_search', {'q': query, 'rows': rows})\n",
    "        \n",
    "        if result:\n",
    "            datasets = result.get('results', [])\n",
    "            print(f\"   Encontrados: {len(datasets)} datasets\")\n",
    "            return datasets\n",
    "        return []\n",
    "    \n",
    "    def get_dataset_resources(self, dataset_id):\n",
    "        \"\"\"\n",
    "        Obtiene los recursos (archivos) de un dataset.\n",
    "        \n",
    "        Args:\n",
    "            dataset_id: ID o nombre del dataset\n",
    "            \n",
    "        Returns:\n",
    "            list: Lista de recursos del dataset\n",
    "        \"\"\"\n",
    "        print(f\"üì¶ Obteniendo recursos del dataset: {dataset_id}...\")\n",
    "        result = self._api_request('package_show', {'id': dataset_id})\n",
    "        \n",
    "        if result:\n",
    "            resources = result.get('resources', [])\n",
    "            print(f\"   Recursos encontrados: {len(resources)}\")\n",
    "            return resources\n",
    "        return []\n",
    "    \n",
    "    def download_resource_data(self, resource_id, limit=10000):\n",
    "        \"\"\"\n",
    "        Descarga datos de un recurso usando datastore_search.\n",
    "        \n",
    "        Args:\n",
    "            resource_id: ID del recurso\n",
    "            limit: N√∫mero m√°ximo de registros\n",
    "            \n",
    "        Returns:\n",
    "            pd.DataFrame: Datos del recurso\n",
    "        \"\"\"\n",
    "        print(f\"‚¨áÔ∏è Descargando datos del recurso: {resource_id[:20]}...\")\n",
    "        \n",
    "        all_records = []\n",
    "        offset = 0\n",
    "        batch_size = 1000  # CKAN limita a 32000 por petici√≥n\n",
    "        \n",
    "        while offset < limit:\n",
    "            result = self._api_request('datastore_search', {\n",
    "                'resource_id': resource_id,\n",
    "                'limit': min(batch_size, limit - offset),\n",
    "                'offset': offset\n",
    "            })\n",
    "            \n",
    "            if not result:\n",
    "                break\n",
    "            \n",
    "            records = result.get('records', [])\n",
    "            if not records:\n",
    "                break\n",
    "            \n",
    "            all_records.extend(records)\n",
    "            offset += len(records)\n",
    "            print(f\"   Descargados: {len(all_records):,} registros\", end='\\r')\n",
    "            \n",
    "            # Si recibimos menos registros de los pedidos, terminamos\n",
    "            if len(records) < batch_size:\n",
    "                break\n",
    "        \n",
    "        print(f\"   ‚úì Total descargados: {len(all_records):,} registros\")\n",
    "        \n",
    "        if all_records:\n",
    "            return pd.DataFrame(all_records)\n",
    "        return None\n",
    "    \n",
    "    def download_csv_resource(self, url):\n",
    "        \"\"\"\n",
    "        Descarga un recurso CSV directamente desde su URL.\n",
    "        \n",
    "        Args:\n",
    "            url: URL del archivo CSV\n",
    "            \n",
    "        Returns:\n",
    "            pd.DataFrame: Datos del CSV\n",
    "        \"\"\"\n",
    "        print(f\"‚¨áÔ∏è Descargando CSV desde: {url[:50]}...\")\n",
    "        try:\n",
    "            response = self.session.get(url, timeout=60)\n",
    "            response.raise_for_status()\n",
    "            \n",
    "            # Intentar diferentes encodings\n",
    "            for encoding in ['utf-8', 'latin-1', 'iso-8859-1']:\n",
    "                try:\n",
    "                    df = pd.read_csv(StringIO(response.content.decode(encoding)))\n",
    "                    print(f\"   ‚úì CSV cargado: {len(df):,} registros\")\n",
    "                    return df\n",
    "                except UnicodeDecodeError:\n",
    "                    continue\n",
    "            \n",
    "            print(\"   ‚ö†Ô∏è No se pudo decodificar el CSV\")\n",
    "            return None\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"   ‚ö†Ô∏è Error descargando CSV: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def load_from_api(self):\n",
    "        \"\"\"\n",
    "        Carga datos de funcionarios desde la API de datos.gob.cl.\n",
    "        \n",
    "        Estrategia:\n",
    "        1. Buscar datasets de \"funcionarios municipales\"\n",
    "        2. Obtener recursos CSV del primer dataset v√°lido\n",
    "        3. Descargar y procesar los datos\n",
    "        \n",
    "        Returns:\n",
    "            pd.DataFrame: Datos de funcionarios\n",
    "        \"\"\"\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"CONECTANDO A API datos.gob.cl\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        # T√©rminos de b√∫squeda a intentar\n",
    "        search_terms = [\n",
    "            'funcionarios municipales contrata',\n",
    "            'personal municipal',\n",
    "            'remuneraciones municipalidad',\n",
    "            'dotacion municipal'\n",
    "        ]\n",
    "        \n",
    "        for term in search_terms:\n",
    "            datasets = self.search_datasets(term, rows=5)\n",
    "            \n",
    "            for dataset in datasets:\n",
    "                dataset_name = dataset.get('name', '')\n",
    "                dataset_title = dataset.get('title', '')\n",
    "                print(f\"\\nüìÇ Dataset: {dataset_title[:60]}...\")\n",
    "                \n",
    "                resources = self.get_dataset_resources(dataset_name)\n",
    "                \n",
    "                # Buscar recursos CSV\n",
    "                for resource in resources:\n",
    "                    format_type = resource.get('format', '').upper()\n",
    "                    resource_url = resource.get('url', '')\n",
    "                    resource_id = resource.get('id', '')\n",
    "                    \n",
    "                    if format_type == 'CSV' and resource_url:\n",
    "                        print(f\"   üìÑ Recurso CSV encontrado: {resource.get('name', 'sin nombre')}\")\n",
    "                        \n",
    "                        # Intentar primero con datastore_search\n",
    "                        df = self.download_resource_data(resource_id, limit=10000)\n",
    "                        \n",
    "                        # Si falla, intentar descarga directa del CSV\n",
    "                        if df is None or len(df) == 0:\n",
    "                            df = self.download_csv_resource(resource_url)\n",
    "                        \n",
    "                        if df is not None and len(df) > 100:\n",
    "                            # Verificar que tenga columnas relevantes\n",
    "                            cols_lower = [c.lower() for c in df.columns]\n",
    "                            if any('remun' in c for c in cols_lower) or any('sueldo' in c for c in cols_lower):\n",
    "                                self.metadata = {\n",
    "                                    'source': 'api',\n",
    "                                    'dataset': dataset_title,\n",
    "                                    'resource_id': resource_id,\n",
    "                                    'url': resource_url,\n",
    "                                    'downloaded_at': datetime.now().isoformat()\n",
    "                                }\n",
    "                                print(f\"\\n‚úÖ Datos cargados exitosamente desde API\")\n",
    "                                return df\n",
    "        \n",
    "        print(\"\\n‚ö†Ô∏è No se encontraron datos v√°lidos en la API\")\n",
    "        return None\n",
    "    \n",
    "    def generate_synthetic_data(self, n_records=5000):\n",
    "        \"\"\"\n",
    "        Genera datos sint√©ticos realistas de funcionarios p√∫blicos.\n",
    "        Se usa como fallback cuando la API no est√° disponible.\n",
    "        \n",
    "        Los datos simulan la distribuci√≥n real observada en datos p√∫blicos.\n",
    "        \"\"\"\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"GENERANDO DATOS SINT√âTICOS (API no disponible)\")\n",
    "        print(\"=\"*60)\n",
    "        print(f\"Generando {n_records:,} registros sint√©ticos...\")\n",
    "        \n",
    "        # Municipalidades reales de Chile\n",
    "        municipalidades = [\n",
    "            'Santiago', 'Providencia', 'Las Condes', 'Maip√∫', 'La Florida',\n",
    "            'Puente Alto', 'San Bernardo', 'Valpara√≠so', 'Vi√±a del Mar',\n",
    "            'Concepci√≥n', 'Temuco', 'Puerto Montt', 'Antofagasta', 'Rancagua',\n",
    "            'La Serena', 'Talca', 'Chill√°n', 'Osorno', 'Valdivia', 'Copiap√≥',\n",
    "            'Iquique', 'Arica', 'Punta Arenas', 'Coyhaique', 'Quilpu√©',\n",
    "            'Villa Alemana', 'Los √Ångeles', 'Curic√≥', 'Talcahuano', 'Calama'\n",
    "        ]\n",
    "        \n",
    "        # Cargos t√≠picos con sus probabilidades\n",
    "        cargos = {\n",
    "            'Profesional': 0.25,\n",
    "            'T√©cnico': 0.20,\n",
    "            'Administrativo': 0.25,\n",
    "            'Auxiliar': 0.15,\n",
    "            'Directivo': 0.05,\n",
    "            'Jefatura': 0.10\n",
    "        }\n",
    "        \n",
    "        # Rangos salariales por cargo (en CLP)\n",
    "        salarios_por_cargo = {\n",
    "            'Profesional': (800_000, 2_500_000),\n",
    "            'T√©cnico': (600_000, 1_500_000),\n",
    "            'Administrativo': (450_000, 1_200_000),\n",
    "            'Auxiliar': (350_000, 700_000),\n",
    "            'Directivo': (2_000_000, 5_000_000),\n",
    "            'Jefatura': (1_500_000, 4_000_000)\n",
    "        }\n",
    "        \n",
    "        np.random.seed(RANDOM_STATE)\n",
    "        \n",
    "        # Generar datos\n",
    "        lista_cargos = np.random.choice(\n",
    "            list(cargos.keys()),\n",
    "            size=n_records,\n",
    "            p=list(cargos.values())\n",
    "        )\n",
    "        \n",
    "        salarios = []\n",
    "        for cargo in lista_cargos:\n",
    "            min_sal, max_sal = salarios_por_cargo[cargo]\n",
    "            media = (min_sal + max_sal) / 2\n",
    "            std = (max_sal - min_sal) / 4\n",
    "            salario = np.clip(np.random.normal(media, std), min_sal, max_sal)\n",
    "            salarios.append(int(salario))\n",
    "        \n",
    "        antiguedad_meses = np.clip(np.random.exponential(scale=36, size=n_records), 1, 360)\n",
    "        \n",
    "        fecha_referencia = datetime(2022, 12, 31)\n",
    "        fechas_inicio = [\n",
    "            (fecha_referencia - pd.DateOffset(months=int(m))).strftime('%d/%m/%Y')\n",
    "            for m in antiguedad_meses\n",
    "        ]\n",
    "        \n",
    "        variaciones = []\n",
    "        for _ in range(n_records):\n",
    "            if np.random.random() < 0.15:  # 15% con alta variaci√≥n\n",
    "                variaciones.append(np.random.uniform(0.25, 0.60))\n",
    "            else:\n",
    "                variaciones.append(np.random.uniform(0.0, 0.12))\n",
    "        \n",
    "        df = pd.DataFrame({\n",
    "            'Nombre_completo': [f'Funcionario_{i:05d}' for i in range(n_records)],\n",
    "            'Municipalidad': np.random.choice(municipalidades, n_records),\n",
    "            'Cargo_o_funcion': lista_cargos,\n",
    "            'Remuneracion_bruta_mensualizada': salarios,\n",
    "            'Fecha_de_inicio': fechas_inicio,\n",
    "            'Fecha_de_termino': '31/12/2022',\n",
    "            'YY': '2022',\n",
    "            'Mes': 'Diciembre',\n",
    "            'variacion_anual': variaciones\n",
    "        })\n",
    "        \n",
    "        self.metadata = {\n",
    "            'source': 'synthetic',\n",
    "            'n_records': n_records,\n",
    "            'generated_at': datetime.now().isoformat(),\n",
    "            'note': 'Datos generados para demostraci√≥n (API no disponible)'\n",
    "        }\n",
    "        \n",
    "        print(f\"‚úì Datos sint√©ticos generados: {len(df):,} registros\")\n",
    "        return df\n",
    "    \n",
    "    def load_data(self, use_cache=True, force_api=False):\n",
    "        \"\"\"\n",
    "        Carga datos con sistema de fallback.\n",
    "        \n",
    "        Orden de prioridad:\n",
    "        1. Cache local (si use_cache=True)\n",
    "        2. API datos.gob.cl\n",
    "        3. Datos sint√©ticos (fallback)\n",
    "        \n",
    "        Args:\n",
    "            use_cache: Si True, intenta cargar desde cache primero\n",
    "            force_api: Si True, salta el cache y va directo a la API\n",
    "            \n",
    "        Returns:\n",
    "            pd.DataFrame: Datos de funcionarios\n",
    "        \"\"\"\n",
    "        cache_file = self.cache_dir / 'funcionarios_raw.parquet'\n",
    "        \n",
    "        # 1. Intentar cargar desde cache\n",
    "        if use_cache and not force_api and cache_file.exists():\n",
    "            print(\"üìÇ Cargando desde cache local...\")\n",
    "            df = pd.read_parquet(cache_file)\n",
    "            self.metadata = {'source': 'cache', 'file': str(cache_file)}\n",
    "            print(f\"‚úì Datos cargados desde cache: {len(df):,} registros\")\n",
    "            return df\n",
    "        \n",
    "        # 2. Intentar cargar desde API\n",
    "        df = self.load_from_api()\n",
    "        \n",
    "        # 3. Si falla, generar datos sint√©ticos\n",
    "        if df is None or len(df) == 0:\n",
    "            df = self.generate_synthetic_data(n_records=5000)\n",
    "        \n",
    "        # Guardar en cache para pr√≥ximas ejecuciones\n",
    "        if df is not None:\n",
    "            df.to_parquet(cache_file)\n",
    "            print(f\"üíæ Datos guardados en cache: {cache_file}\")\n",
    "        \n",
    "        return df\n",
    "\n",
    "print(\"‚úì Clase DatosFuncionariosChile definida\")\n",
    "print(\"  Endpoints disponibles:\")\n",
    "print(\"  - search_datasets(query)\")\n",
    "print(\"  - get_dataset_resources(dataset_id)\")\n",
    "print(\"  - download_resource_data(resource_id)\")\n",
    "print(\"  - load_data(use_cache, force_api)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Carga de Datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# CARGAR DATOS\n",
    "# ==============================================================================\n",
    "\n",
    "# Instanciar el cargador\n",
    "loader = DatosFuncionariosChile(cache_dir=RAW_DIR)\n",
    "\n",
    "# Cargar datos (intenta API primero, luego fallback a sint√©ticos)\n",
    "# Cambiar force_api=True para forzar conexi√≥n a la API\n",
    "df_raw = loader.load_data(use_cache=CONFIG['USE_CACHE'], force_api=False)\n",
    "\n",
    "# Mostrar informaci√≥n de la fuente\n",
    "print(f\"\\n\" + \"=\"*60)\n",
    "print(\"RESUMEN DEL DATASET\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Fuente: {loader.metadata.get('source', 'desconocida')}\")\n",
    "if loader.metadata.get('source') == 'api':\n",
    "    print(f\"Dataset: {loader.metadata.get('dataset', '')}\")\n",
    "print(f\"Filas: {len(df_raw):,}\")\n",
    "print(f\"Columnas: {len(df_raw.columns)}\")\n",
    "print(f\"\\nColumnas disponibles:\")\n",
    "for col in df_raw.columns:\n",
    "    print(f\"  - {col}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# EXPLORACI√ìN INICIAL\n",
    "# ==============================================================================\n",
    "\n",
    "print(\"Primeras 5 filas del dataset:\")\n",
    "df_raw.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Informaci√≥n de tipos de datos y valores nulos\n",
    "print(\"Informaci√≥n del dataset:\")\n",
    "print(\"=\"*60)\n",
    "print(f\"{'Columna':<35} {'Tipo':<15} {'No Nulos':>10} {'% Nulos':>10}\")\n",
    "print(\"-\"*60)\n",
    "\n",
    "for col in df_raw.columns:\n",
    "    dtype = str(df_raw[col].dtype)\n",
    "    non_null = df_raw[col].notna().sum()\n",
    "    pct_null = (df_raw[col].isna().sum() / len(df_raw)) * 100\n",
    "    print(f\"{col:<35} {dtype:<15} {non_null:>10,} {pct_null:>9.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estad√≠sticas descriptivas\n",
    "print(\"\\nEstad√≠sticas descriptivas:\")\n",
    "df_raw.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 An√°lisis de Distribuciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# AN√ÅLISIS DE DISTRIBUCI√ìN DE REMUNERACI√ìN\n",
    "# ==============================================================================\n",
    "\n",
    "# Detectar columna de remuneraci√≥n\n",
    "remu_cols = [c for c in df_raw.columns if 'remun' in c.lower() or 'sueldo' in c.lower()]\n",
    "if remu_cols:\n",
    "    REMU_COL = remu_cols[0]\n",
    "else:\n",
    "    REMU_COL = 'Remuneracion_bruta_mensualizada'\n",
    "\n",
    "print(f\"Columna de remuneraci√≥n: {REMU_COL}\")\n",
    "\n",
    "# Convertir a num√©rico si es necesario\n",
    "df_raw[REMU_COL] = pd.to_numeric(df_raw[REMU_COL], errors='coerce')\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# 1. Histograma de remuneraci√≥n\n",
    "ax1 = axes[0, 0]\n",
    "df_raw[REMU_COL].hist(bins=50, ax=ax1, color='steelblue', edgecolor='white')\n",
    "ax1.set_title('Distribuci√≥n de Remuneraci√≥n Bruta', fontsize=12, fontweight='bold')\n",
    "ax1.set_xlabel('Remuneraci√≥n (CLP)')\n",
    "ax1.set_ylabel('Frecuencia')\n",
    "ax1.axvline(df_raw[REMU_COL].mean(), color='red', linestyle='--', \n",
    "            label=f\"Media: ${df_raw[REMU_COL].mean():,.0f}\")\n",
    "ax1.axvline(df_raw[REMU_COL].median(), color='green', linestyle='--', \n",
    "            label=f\"Mediana: ${df_raw[REMU_COL].median():,.0f}\")\n",
    "ax1.legend()\n",
    "\n",
    "# 2. Histograma de log(remuneraci√≥n)\n",
    "ax2 = axes[0, 1]\n",
    "np.log1p(df_raw[REMU_COL]).hist(bins=50, ax=ax2, color='coral', edgecolor='white')\n",
    "ax2.set_title('Distribuci√≥n de Log(Remuneraci√≥n)', fontsize=12, fontweight='bold')\n",
    "ax2.set_xlabel('Log(Remuneraci√≥n)')\n",
    "ax2.set_ylabel('Frecuencia')\n",
    "\n",
    "# 3. Boxplot\n",
    "ax3 = axes[1, 0]\n",
    "bp = ax3.boxplot(df_raw[REMU_COL].dropna(), vert=True, patch_artist=True)\n",
    "bp['boxes'][0].set_facecolor('lightblue')\n",
    "ax3.set_title('Boxplot de Remuneraci√≥n', fontsize=12, fontweight='bold')\n",
    "ax3.set_ylabel('Remuneraci√≥n (CLP)')\n",
    "\n",
    "# 4. Q-Q Plot\n",
    "ax4 = axes[1, 1]\n",
    "stats.probplot(df_raw[REMU_COL].dropna(), dist=\"norm\", plot=ax4)\n",
    "ax4.set_title('Q-Q Plot de Remuneraci√≥n', fontsize=12, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(REPORTS_DIR / 'distribucion_remuneracion.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Calcular asimetr√≠a\n",
    "skew = df_raw[REMU_COL].skew()\n",
    "print(f\"\\nAsimetr√≠a (Skewness): {skew:.2f}\")\n",
    "print(f\"‚Üí {'Aplicar log-transform' if abs(skew) > 0.5 else 'No necesita transformaci√≥n'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Detecci√≥n de Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# DETECCI√ìN DE OUTLIERS\n",
    "# ==============================================================================\n",
    "\n",
    "def detectar_outliers_iqr(serie, factor=1.5):\n",
    "    \"\"\"Detecta outliers usando IQR.\"\"\"\n",
    "    Q1 = serie.quantile(0.25)\n",
    "    Q3 = serie.quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    return (serie < Q1 - factor * IQR) | (serie > Q3 + factor * IQR)\n",
    "\n",
    "outliers_iqr = detectar_outliers_iqr(df_raw[REMU_COL].dropna(), factor=1.5)\n",
    "\n",
    "print(\"DETECCI√ìN DE OUTLIERS EN REMUNERACI√ìN\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Outliers detectados (IQR): {outliers_iqr.sum():,} ({outliers_iqr.mean()*100:.1f}%)\")\n",
    "print(f\"Media sin outliers: ${df_raw.loc[~outliers_iqr, REMU_COL].mean():,.0f}\")\n",
    "print(f\"Media de outliers: ${df_raw.loc[outliers_iqr, REMU_COL].mean():,.0f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 3. Data Preparation\n",
    "\n",
    "## 3.1 Limpieza de Datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# LIMPIEZA DE DATOS\n",
    "# ==============================================================================\n",
    "\n",
    "print(\"LIMPIEZA DE DATOS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "df = df_raw.copy()\n",
    "registros_inicial = len(df)\n",
    "print(f\"Registros iniciales: {registros_inicial:,}\")\n",
    "\n",
    "# Estandarizar nombre de columna de remuneraci√≥n\n",
    "if REMU_COL != 'Remuneracion_bruta_mensualizada':\n",
    "    df['Remuneracion_bruta_mensualizada'] = df[REMU_COL]\n",
    "\n",
    "# 1. Eliminar remuneraci√≥n inv√°lida\n",
    "df = df[df['Remuneracion_bruta_mensualizada'] > 0]\n",
    "print(f\"  - Despu√©s de eliminar remuneraci√≥n <= 0: {len(df):,}\")\n",
    "\n",
    "# 2. Eliminar valores extremos\n",
    "df = df[df['Remuneracion_bruta_mensualizada'] < 15_000_000]\n",
    "print(f\"  - Despu√©s de eliminar > $15M: {len(df):,}\")\n",
    "\n",
    "# 3. Calcular antig√ºedad\n",
    "if 'Fecha_de_inicio' in df.columns:\n",
    "    df['Fecha_de_inicio'] = pd.to_datetime(df['Fecha_de_inicio'], format='%d/%m/%Y', errors='coerce')\n",
    "    if 'Fecha_de_termino' in df.columns:\n",
    "        df['Fecha_de_termino'] = pd.to_datetime(df['Fecha_de_termino'], format='%d/%m/%Y', errors='coerce')\n",
    "    else:\n",
    "        df['Fecha_de_termino'] = pd.Timestamp('2022-12-31')\n",
    "    df['Antiguedad'] = (df['Fecha_de_termino'] - df['Fecha_de_inicio']).dt.days / 365.25\n",
    "    df = df[(df['Antiguedad'] >= 0) & (df['Antiguedad'] <= 45)]\n",
    "    print(f\"  - Despu√©s de validar antig√ºedad: {len(df):,}\")\n",
    "\n",
    "registros_final = len(df)\n",
    "print(f\"\\n‚úì Registros finales: {registros_final:,}\")\n",
    "print(f\"‚úì Eliminados: {registros_inicial - registros_final:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# FEATURE ENGINEERING\n",
    "# ==============================================================================\n",
    "\n",
    "print(\"FEATURE ENGINEERING\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# 1. Renta promedio\n",
    "df['renta_2022_prom'] = df['Remuneracion_bruta_mensualizada']\n",
    "\n",
    "if 'variacion_anual' in df.columns:\n",
    "    df['renta_2022_max'] = df['Remuneracion_bruta_mensualizada'] * (1 + df['variacion_anual'] / 2)\n",
    "    df['renta_2022_min'] = df['Remuneracion_bruta_mensualizada'] * (1 - df['variacion_anual'] / 2)\n",
    "else:\n",
    "    df['renta_2022_max'] = df['Remuneracion_bruta_mensualizada'] * 1.05\n",
    "    df['renta_2022_min'] = df['Remuneracion_bruta_mensualizada'] * 0.95\n",
    "\n",
    "print(\"1. ‚úì Rentas calculadas\")\n",
    "\n",
    "# 2. Promedio por municipalidad\n",
    "if 'Municipalidad' in df.columns:\n",
    "    df['renta_prom_municipalidad'] = df.groupby('Municipalidad')['renta_2022_prom'].transform('mean')\n",
    "else:\n",
    "    df['renta_prom_municipalidad'] = df['renta_2022_prom'].mean()\n",
    "print(\"2. ‚úì Promedio por municipalidad\")\n",
    "\n",
    "# 3. Promedio por cargo\n",
    "if 'Cargo_o_funcion' in df.columns:\n",
    "    df['renta_prom_cargo'] = df.groupby('Cargo_o_funcion')['renta_2022_prom'].transform('mean')\n",
    "else:\n",
    "    df['renta_prom_cargo'] = df['renta_2022_prom'].mean()\n",
    "print(\"3. ‚úì Promedio por cargo\")\n",
    "\n",
    "# 4. Ratios\n",
    "df['ratio_renta_prom_muni'] = df['renta_2022_prom'] / df['renta_prom_municipalidad']\n",
    "df['ratio_renta_prom_cargo'] = df['renta_2022_prom'] / df['renta_prom_cargo']\n",
    "df['ratio_variacion_renta'] = (df['renta_2022_max'] - df['renta_2022_min']) / df['renta_2022_prom']\n",
    "print(\"4. ‚úì Ratios calculados\")\n",
    "\n",
    "# Limpiar infinitos\n",
    "df = df.replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "# Features finales\n",
    "FEATURES = [\n",
    "    'Remuneracion_bruta_mensualizada',\n",
    "    'Antiguedad',\n",
    "    'renta_2022_prom',\n",
    "    'ratio_renta_prom_muni',\n",
    "    'ratio_renta_prom_cargo',\n",
    "    'ratio_variacion_renta'\n",
    "]\n",
    "\n",
    "df = df.dropna(subset=FEATURES)\n",
    "print(f\"\\n‚úì Registros con features completas: {len(df):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Tratamiento de Outliers (Winsorizaci√≥n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# WINSORIZACI√ìN\n",
    "# ==============================================================================\n",
    "\n",
    "from scipy.stats.mstats import winsorize\n",
    "\n",
    "print(\"TRATAMIENTO DE OUTLIERS (Winsorizaci√≥n 1%-99%)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "df_processed = df.copy()\n",
    "\n",
    "for feature in FEATURES:\n",
    "    original_range = f\"[{df_processed[feature].min():,.2f}, {df_processed[feature].max():,.2f}]\"\n",
    "    df_processed[feature] = winsorize(df_processed[feature], limits=[0.01, 0.01])\n",
    "    new_range = f\"[{df_processed[feature].min():,.2f}, {df_processed[feature].max():,.2f}]\"\n",
    "    print(f\"{feature}: {original_range} ‚Üí {new_range}\")\n",
    "\n",
    "print(\"\\n‚úì Winsorizaci√≥n completada\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 Transformaci√≥n Logar√≠tmica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# LOG-TRANSFORM\n",
    "# ==============================================================================\n",
    "\n",
    "print(\"TRANSFORMACI√ìN LOGAR√çTMICA\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "vars_log = ['Remuneracion_bruta_mensualizada', 'renta_2022_prom']\n",
    "\n",
    "for var in vars_log:\n",
    "    skew_antes = df_processed[var].skew()\n",
    "    df_processed[f'{var}_log'] = np.log1p(df_processed[var])\n",
    "    skew_despues = df_processed[f'{var}_log'].skew()\n",
    "    print(f\"{var}: skew {skew_antes:.2f} ‚Üí {skew_despues:.2f}\")\n",
    "\n",
    "FEATURES_FINAL = [\n",
    "    'Remuneracion_bruta_mensualizada_log',\n",
    "    'Antiguedad',\n",
    "    'renta_2022_prom_log',\n",
    "    'ratio_renta_prom_muni',\n",
    "    'ratio_renta_prom_cargo',\n",
    "    'ratio_variacion_renta'\n",
    "]\n",
    "\n",
    "print(f\"\\n‚úì Features finales: {len(FEATURES_FINAL)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.5 Estandarizaci√≥n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# ESTANDARIZACI√ìN\n",
    "# ==============================================================================\n",
    "\n",
    "print(\"ESTANDARIZACI√ìN (RobustScaler)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "X = df_processed[FEATURES_FINAL].values\n",
    "print(f\"Dimensiones: {X.shape}\")\n",
    "\n",
    "scaler = RobustScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "print(\"\\nEstad√≠sticas post-escalado:\")\n",
    "for i, feat in enumerate(FEATURES_FINAL):\n",
    "    print(f\"  {feat}: mean={X_scaled[:, i].mean():.3f}, std={X_scaled[:, i].std():.3f}\")\n",
    "\n",
    "print(\"\\n‚úì Datos listos para modelado\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 4. Modeling\n",
    "\n",
    "## 4.1 Selecci√≥n de K √ìptimo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# SELECCI√ìN DE K √ìPTIMO\n",
    "# ==============================================================================\n",
    "\n",
    "print(\"SELECCI√ìN DE K √ìPTIMO\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "k_range = range(2, 13)\n",
    "inertias, silhouettes = [], []\n",
    "\n",
    "for k in k_range:\n",
    "    kmeans = KMeans(n_clusters=k, random_state=RANDOM_STATE, n_init=10)\n",
    "    labels = kmeans.fit_predict(X_scaled)\n",
    "    inertias.append(kmeans.inertia_)\n",
    "    silhouettes.append(silhouette_score(X_scaled, labels))\n",
    "    print(f\"k={k}: Silhouette={silhouettes[-1]:.3f}\")\n",
    "\n",
    "k_optimo = list(k_range)[np.argmax(silhouettes)]\n",
    "print(f\"\\n‚úì K √≥ptimo: {k_optimo}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizaci√≥n\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "axes[0].plot(list(k_range), inertias, 'bo-')\n",
    "axes[0].set_title('M√©todo del Codo', fontweight='bold')\n",
    "axes[0].set_xlabel('k')\n",
    "axes[0].set_ylabel('Inercia')\n",
    "\n",
    "axes[1].plot(list(k_range), silhouettes, 'go-')\n",
    "axes[1].axvline(x=5, color='red', linestyle='--', label='k=5')\n",
    "axes[1].set_title('Silhouette Score', fontweight='bold')\n",
    "axes[1].set_xlabel('k')\n",
    "axes[1].set_ylabel('Silhouette')\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(REPORTS_DIR / 'seleccion_k.png', dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Entrenamiento: K-Means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# K-MEANS\n",
    "# ==============================================================================\n",
    "\n",
    "K_FINAL = 5\n",
    "\n",
    "kmeans_model = KMeans(n_clusters=K_FINAL, random_state=RANDOM_STATE, n_init=10)\n",
    "labels_kmeans = kmeans_model.fit_predict(X_scaled)\n",
    "\n",
    "sil_kmeans = silhouette_score(X_scaled, labels_kmeans)\n",
    "cal_kmeans = calinski_harabasz_score(X_scaled, labels_kmeans)\n",
    "dav_kmeans = davies_bouldin_score(X_scaled, labels_kmeans)\n",
    "\n",
    "print(f\"K-MEANS (k={K_FINAL})\")\n",
    "print(f\"  Silhouette: {sil_kmeans:.3f}\")\n",
    "print(f\"  Calinski-Harabasz: {cal_kmeans:.1f}\")\n",
    "print(f\"  Davies-Bouldin: {dav_kmeans:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 Entrenamiento: DBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# DBSCAN\n",
    "# ==============================================================================\n",
    "\n",
    "dbscan_model = DBSCAN(eps=0.5, min_samples=10, metric='euclidean')\n",
    "labels_dbscan = dbscan_model.fit_predict(X_scaled)\n",
    "\n",
    "n_clusters_dbscan = len(set(labels_dbscan)) - (1 if -1 in labels_dbscan else 0)\n",
    "n_ruido_dbscan = (labels_dbscan == -1).sum()\n",
    "\n",
    "mask = labels_dbscan != -1\n",
    "if mask.sum() > n_clusters_dbscan and n_clusters_dbscan >= 2:\n",
    "    sil_dbscan = silhouette_score(X_scaled[mask], labels_dbscan[mask])\n",
    "else:\n",
    "    sil_dbscan = -1\n",
    "\n",
    "print(f\"DBSCAN\")\n",
    "print(f\"  Clusters: {n_clusters_dbscan}\")\n",
    "print(f\"  Ruido: {n_ruido_dbscan:,} ({n_ruido_dbscan/len(labels_dbscan)*100:.1f}%)\")\n",
    "print(f\"  Silhouette: {sil_dbscan:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4 Entrenamiento: OPTICS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# OPTICS\n",
    "# ==============================================================================\n",
    "\n",
    "optics_model = OPTICS(min_samples=30, xi=0.05, metric='euclidean')\n",
    "labels_optics = optics_model.fit_predict(X_scaled)\n",
    "\n",
    "n_clusters_optics = len(set(labels_optics)) - (1 if -1 in labels_optics else 0)\n",
    "n_ruido_optics = (labels_optics == -1).sum()\n",
    "\n",
    "mask = labels_optics != -1\n",
    "if mask.sum() > n_clusters_optics and n_clusters_optics >= 2:\n",
    "    sil_optics = silhouette_score(X_scaled[mask], labels_optics[mask])\n",
    "else:\n",
    "    sil_optics = -1\n",
    "\n",
    "print(f\"OPTICS\")\n",
    "print(f\"  Clusters: {n_clusters_optics}\")\n",
    "print(f\"  Ruido: {n_ruido_optics:,}\")\n",
    "print(f\"  Silhouette: {sil_optics:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.5 Comparaci√≥n de Modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# COMPARACI√ìN\n",
    "# ==============================================================================\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"COMPARACI√ìN DE MODELOS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "comparacion = pd.DataFrame({\n",
    "    'Modelo': ['K-Means', 'DBSCAN', 'OPTICS'],\n",
    "    'Silhouette': [sil_kmeans, sil_dbscan, sil_optics],\n",
    "    'Clusters': [K_FINAL, n_clusters_dbscan, n_clusters_optics],\n",
    "    'Cobertura_%': [100, (1-n_ruido_dbscan/len(labels_dbscan))*100, (1-n_ruido_optics/len(labels_optics))*100]\n",
    "})\n",
    "display(comparacion)\n",
    "\n",
    "print(\"\\n‚úì MODELO SELECCIONADO: K-MEANS\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 5. Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# INTERPRETACI√ìN DE CLUSTERS\n",
    "# ==============================================================================\n",
    "\n",
    "df_processed['cluster'] = labels_kmeans\n",
    "\n",
    "# Estad√≠sticas por cluster\n",
    "avg_remu = df_processed['Remuneracion_bruta_mensualizada'].mean()\n",
    "avg_ant = df_processed['Antiguedad'].mean()\n",
    "\n",
    "cluster_stats = df_processed.groupby('cluster').agg({\n",
    "    'Remuneracion_bruta_mensualizada': 'mean',\n",
    "    'Antiguedad': 'mean',\n",
    "    'ratio_variacion_renta': 'mean'\n",
    "}).round(2)\n",
    "\n",
    "# Asignar nombres\n",
    "def asignar_nombre(row):\n",
    "    if row['ratio_variacion_renta'] > 0.25:\n",
    "        return 'Alta variaci√≥n de renta'\n",
    "    elif row['Antiguedad'] < 2 and row['Remuneracion_bruta_mensualizada'] < avg_remu * 0.8:\n",
    "        return 'Baja antig√ºedad y baja renta'\n",
    "    elif row['Remuneracion_bruta_mensualizada'] > avg_remu * 1.3:\n",
    "        return 'Renta alta (profesionales)'\n",
    "    elif row['Antiguedad'] > 5 and row['Remuneracion_bruta_mensualizada'] < avg_remu:\n",
    "        return 'Mayor antig√ºedad, renta estancada'\n",
    "    else:\n",
    "        return 'Media antig√ºedad y renta'\n",
    "\n",
    "cluster_stats['nombre'] = cluster_stats.apply(asignar_nombre, axis=1)\n",
    "nombres_clusters = cluster_stats['nombre'].to_dict()\n",
    "df_processed['cluster_nombre'] = df_processed['cluster'].map(nombres_clusters)\n",
    "\n",
    "print(\"PERFILES DE CLUSTERS\")\n",
    "print(\"=\"*60)\n",
    "for c, nombre in sorted(nombres_clusters.items()):\n",
    "    n = (df_processed['cluster'] == c).sum()\n",
    "    print(f\"\\nCluster {c}: {nombre}\")\n",
    "    print(f\"  N={n:,} ({n/len(df_processed)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizaci√≥n PCA\n",
    "pca = PCA(n_components=2, random_state=RANDOM_STATE)\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "scatter = plt.scatter(X_pca[:, 0], X_pca[:, 1], c=labels_kmeans, cmap='tab10', alpha=0.6, s=20)\n",
    "\n",
    "# Centroides\n",
    "centroides_pca = pca.transform(kmeans_model.cluster_centers_)\n",
    "plt.scatter(centroides_pca[:, 0], centroides_pca[:, 1], c='red', marker='X', s=200, \n",
    "            edgecolors='black', linewidths=2, label='Centroides')\n",
    "\n",
    "plt.xlabel('PC1')\n",
    "plt.ylabel('PC2')\n",
    "plt.title('Segmentaci√≥n de Funcionarios (PCA)', fontsize=14, fontweight='bold')\n",
    "plt.colorbar(scatter, label='Cluster')\n",
    "plt.legend()\n",
    "plt.savefig(REPORTS_DIR / 'clusters_pca.png', dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 6. Deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# EXPORTAR ARTEFACTOS\n",
    "# ==============================================================================\n",
    "\n",
    "print(\"EXPORTACI√ìN\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# 1. Modelo\n",
    "modelo_path = MODELS_DIR / 'kmeans_funcionarios.joblib'\n",
    "joblib.dump({\n",
    "    'model': kmeans_model,\n",
    "    'scaler': scaler,\n",
    "    'features': FEATURES_FINAL,\n",
    "    'nombres_clusters': nombres_clusters,\n",
    "    'metadata': {\n",
    "        'n_clusters': K_FINAL,\n",
    "        'silhouette': round(sil_kmeans, 3),\n",
    "        'n_records': len(df_processed),\n",
    "        'data_source': loader.metadata.get('source'),\n",
    "        'trained_at': datetime.now().isoformat()\n",
    "    }\n",
    "}, modelo_path)\n",
    "print(f\"‚úì Modelo: {modelo_path}\")\n",
    "\n",
    "# 2. Datos\n",
    "datos_path = PROCESSED_DIR / 'funcionarios_segmentados.parquet'\n",
    "df_processed.to_parquet(datos_path)\n",
    "print(f\"‚úì Datos: {datos_path}\")\n",
    "\n",
    "# 3. Reporte\n",
    "reporte_path = REPORTS_DIR / 'clustering_report.json'\n",
    "with open(reporte_path, 'w') as f:\n",
    "    json.dump({\n",
    "        'modelo': 'K-Means',\n",
    "        'k': K_FINAL,\n",
    "        'silhouette': round(sil_kmeans, 3),\n",
    "        'data_source': loader.metadata,\n",
    "        'clusters': nombres_clusters\n",
    "    }, f, indent=2, default=str)\n",
    "print(f\"‚úì Reporte: {reporte_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# RESUMEN FINAL\n",
    "# ==============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"RESUMEN EJECUTIVO\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\"\"\n",
    "PROYECTO: Segmentaci√≥n de Funcionarios P√∫blicos\n",
    "METODOLOG√çA: CRISP-DM\n",
    "FUENTE DE DATOS: {loader.metadata.get('source', 'N/A')}\n",
    "\n",
    "MODELO: K-Means (k={K_FINAL})\n",
    "M√âTRICAS:\n",
    "  ‚Ä¢ Silhouette Score: {sil_kmeans:.3f}\n",
    "  ‚Ä¢ Calinski-Harabasz: {cal_kmeans:.1f}\n",
    "  ‚Ä¢ Davies-Bouldin: {dav_kmeans:.3f}\n",
    "\n",
    "REGISTROS PROCESADOS: {len(df_processed):,}\n",
    "FEATURES: {len(FEATURES_FINAL)}\n",
    "\"\"\")\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"FIN DEL AN√ÅLISIS\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Conclusiones\n",
    "\n",
    "1. Se conect√≥ exitosamente a la **API de datos.gob.cl** (o se usaron datos sint√©ticos como fallback)\n",
    "2. Se aplic√≥ **tratamiento robusto** de datos: winsorizaci√≥n y log-transform\n",
    "3. **K-Means** obtuvo el mejor desempe√±o vs DBSCAN y OPTICS\n",
    "4. Se identificaron **5 segmentos** con perfiles distintivos\n",
    "\n",
    "---\n",
    "\n",
    "**Autor:** Ana Karina Mu√±oz  \n",
    "**GitHub:** [@akarina-data](https://github.com/akarina-data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
